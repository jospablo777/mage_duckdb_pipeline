---
title: "Building a data pipeline with Mage, Polars, and DuckDB"
include-in-header:
  - text: |
      <link rel = "shortcut icon" href = "img/favicon.png" />
author:
  name: "José P. Barrantes"
  url: "https://www.linkedin.com/in/jose-barrantes/"
description: "GitHub Repo: [SODA to DuckDB](https://github.com/jospablo777/)"
date: "`r Sys.Date()`"
published-title: "Last updated"
format: 
  html:
    toc: true
    toc-location: right
    toc-title: "On this page"
    aside: true
editor: source
---

```{r}
#| label: setup
#| include: false
#| output: false
#| eval: true

# Tell quarto where is our python venv
Sys.setenv(RETICULATE_PYTHON = "../venv/bin/python")
```

![](img/bonjour_bear.png)

## Where are we in the data lifecycle?

As data analysts or scientists, we often find ourselves working downstream in the data lifecycle. Most of the time, our role involves transforming and analyzing data that has already been prepared and served to us by upstream processes. However, having a deeper understanding of the entire data pipeline—from ingestion to transformation and storage—can empower us to optimize workflows, ensure data quality, and unlock new insights. 

![The data lifecycle and where you are. Modified from "Fundamentals of Data Engineering" by Reis & Housley (2022).](img/data_lifecycle.png)

Another advantage of gaining understanding—and hands-on experience—in data engineering processes is the **empathy** we build with our data engineers. These are the colleagues we work closely with and rely on, making a strong, **collaborative relationship** essential. Hence, in this hands-on article, we will explore the Python ecosystem by examining tools such as Mage, Polars, and DuckDB. We'll demonstrate how these tools can help us build efficient, lightweight data pipelines that take data from the source and store it in a format that is well-suited for high-performance analytics.

## About the tools

In this project, we will use Mage as our data orchestrator. The tasks managed by the orchestrator will include data manipulation and transformation using Polars, as well as persistent storage with DuckDB.

### What are data orchestrators?

Data orchestrators, such as Mage, are tools that help manage, schedule, and monitor workflows in data pipelines. They allow us to automate complex processes, ensuring that tasks are executed in the correct order and dependencies are handled seamlessly. By using Mage as our orchestrator, we can streamline our data pipeline and focus on building efficient workflows that allow us to set a local database appropriate for our analysis.

### Data manipulation and storage

Why Polars and DuckDB? The answer lies in the size and nature of the dataset we're working with. Since our dataset is small enough to fit in memory, we don't need a distributed system like Spark. Polars, with its fast and memory-efficient operations, is perfect for data manipulation and transformation. Meanwhile, DuckDB provides a lightweight, yet powerful, SQL-based engine for persistent storage and querying. Together, these tools offer a simple, performant, and highly efficient solution for handling our data pipeline.

Returning to the data lifecycle diagram, we can land it in a more concrete way to show how our project will be built and executed:

![The data lifecycle and a more concrete overview of what **we will implement** in this project. Modified from "Fundamentals of Data Engineering" by Reis & Housley (2022).](img/data_lifecycle_logos.png)

First, we will fetch the data from the Socrata Open Data API (SODA) through HTTP. After that, we will generate some extra variables of our interest with Polars, to finally store it in a DuckDB database we will consume for analytics and predictive modeling. All this is orchestrated with Mage.

### Tools and resources overview:

- [**SODA API:**](https://dev.socrata.com/) provides access to open datasets, serving as our data source. It contains "a wealth of open data resources from governments, non-profits, and NGOs around the world[^1]."
- [**Mage:**](https://www.mage.ai/) the data orchestrator that will automate and manage the pipeline.
- [**Polars:**](https://pola.rs/) a high-performance data frame library implemented in Rust, ideal for fast data manipulation.
- [**DuckDB:**](https://duckdb.org/) a columnar database system designed for efficient analytics and in-memory processing.


We've chosen the [Iowa Liquor Sales](https://data.iowa.gov/Sales-Distribution/Iowa-Liquor-Sales/m3tr-qhgy/about_data) dataset since it is big enough to make this (extract, transform, load) ETL process interesting.

[^1]: According to their web page :)


## Our problem

Let's say we work for a big chain of liquor stores in the US, Iowa. Part of the intelligence in your company is built upon the information made available through SODA API, and it feeds some of the dashboards the decision-makers consume. You also use it often to do research and predictive modeling. Some stakeholders have started complaining about the loading times of the dashboards, and you have also been a little frustrated with the time it takes to get the data to train your predictive models.

![](img/predictive_modeling_simpsons.png)





Token

```{python}
#| label: token_mention
#| include: true
#| eval: false

headers = {"X-App-Token": "YOUR_TOKEN"}
response = requests.get(data_url, headers=headers)
```

Bash execution

```{bash}
#| label: bash_pipeline_output
#| include: true
#| eval: false

user@user-pc:~/mage_duckdb_pipeline$ mage run . socrata_iowa_liquor_pipeline
Fetching the records-per-year metadata. This might take a couple minutes..
Done! We have our year record metadata.

SODA data pull started.
Years to be fetched: 2017, 2018, 2019, 2020, 2021.
Fetching data: 100%|█████████████████████████████████████| 5/5 [10:08<00:00, 121.69s/it]
Product-related new variables, generated.
Sales and price related metrics, computed.
Volume-based features, computed.
Time-based features, computed.
Data loaded to your DuckDB database!
Pipeline run completed.
```


```{r}
conn2 <- DBI::dbConnect(
  drv = duckdb::duckdb(),
  dbdir = "../data/iowa_liquor.duckdb",
  read_only = TRUE
)
```

```{sql}
#| output.var: query2
#| echo: true
#| output: true
#| connection: conn2

SELECT *
FROM iowa_liquor_sales
LIMIT 10
```
```{r}
tibble::tibble(query2)
```

```{python}
import duckdb

con_py = duckdb.connect("../data/iowa_liquor.duckdb", read_only=True)

polars_df = con_py.sql("SELECT liquor_type, bottle_size, price_per_liter FROM iowa_liquor_sales LIMIT 10").pl()

polars_df
```
```{python}
polars_df.height
```

## About the polar bear in the header

https://knowyourmeme.com/memes/bonjour-bear

## Cited works and recommended readings

- Reis, J., & Housley, M. (2022). *Fundamentals of data engineering: Plan and build robust data systems*. O'Reilly Media.