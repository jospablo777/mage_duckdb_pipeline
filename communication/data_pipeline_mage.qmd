---
title: "Building a data pipeline with Mage, Polars, and DuckDB"
include-in-header:
  - text: |
      <link rel = "shortcut icon" href = "img/favicon.png" />
author:
  name: "Jos√© P. Barrantes"
  url: "https://www.linkedin.com/in/jose-barrantes/"
description: "GitHub Repo: [SODA to DuckDB](https://github.com/jospablo777/)"
date: "`r Sys.Date()`"
published-title: "Last updated"
format: 
  html:
    toc: true
    toc-location: right
    toc-title: "On this page"
    aside: true
editor: source
---

```{r}
#| label: setup
#| include: false
#| output: false
#| eval: true

# Prelude libraries
library(readr)
library(dplyr)
library(ggplot2)

# Tell quarto where is our python venv
Sys.setenv(RETICULATE_PYTHON = "../venv/bin/python")
```

![](img/bonjour_bear.png)

## Where are we in the data lifecycle?

As data analysts or scientists, we often find ourselves working downstream in the data lifecycle. Most of the time, our role involves transforming and analyzing data that has already been prepared and served to us by upstream processes. However, having a deeper understanding of the entire data pipeline‚Äîfrom ingestion to transformation and storage‚Äîcan empower us to optimize workflows, ensure data quality, and unlock new insights. 

![The data lifecycle and where you are. Modified from "Fundamentals of Data Engineering" by Reis & Housley (2022).](img/data_lifecycle.png)

Another advantage of gaining understanding‚Äîand hands-on experience‚Äîin data engineering processes is the **empathy** we build with our data engineers. These are the colleagues we work closely with and rely on, making a strong, **collaborative relationship** essential. Hence, in this hands-on article, we will explore the Python ecosystem by examining tools such as Mage, Polars, and DuckDB. We'll demonstrate how these tools can help us build efficient, lightweight data pipelines that take data from the source and store it in a format that is well-suited for high-performance analytics.

## About the tools

In this project, we will use Mage as our data orchestrator. The tasks managed by the orchestrator will include data manipulation and transformation using Polars, as well as persistent storage with DuckDB.

### What are data orchestrators?

Data orchestrators, such as Mage, are tools that help manage, schedule, and monitor workflows in data pipelines. They allow us to automate complex processes, ensuring that tasks are executed in the correct order and dependencies are handled seamlessly. By using Mage as our orchestrator, we can streamline our data pipeline and focus on building efficient workflows that allow us to set a local database appropriate for our analysis.

### Data manipulation and storage

Why Polars and DuckDB? The answer lies in the size and nature of the dataset we're working with. Since our dataset is small enough to fit in memory, we don't need a distributed system like Spark. Polars, with its fast and memory-efficient operations, is perfect for data manipulation and transformation. Meanwhile, DuckDB provides a lightweight, yet powerful, SQL-based engine for persistent storage and querying. Together, these tools offer a simple, performant, and highly efficient solution for handling our data pipeline.

Returning to the data lifecycle diagram, we can land it in a more concrete way to show how our project will be built and executed:

![The data lifecycle and a more concrete overview of what **we will implement** in this project. Modified from "Fundamentals of Data Engineering" by Reis & Housley (2022).](img/data_lifecycle_logos.png){#fig-concrete-project-outline}

First, we will fetch the data from the Socrata Open Data API (SODA) through HTTP. After that, we will generate some extra variables of our interest with Polars, to finally store it in a DuckDB database we will consume for analytics and predictive modeling. All this is orchestrated with Mage.

### Tools and resources overview:

- [**SODA API:**](https://dev.socrata.com/) provides access to open datasets, serving as our data source. It contains "a wealth of open data resources from governments, non-profits, and NGOs around the world[^1]."
- [**Mage:**](https://www.mage.ai/) the data orchestrator that will automate and manage the pipeline.
- [**Polars:**](https://pola.rs/) a high-performance data frame library implemented in Rust, ideal for fast data manipulation.
- [**DuckDB:**](https://duckdb.org/) a columnar database system designed for efficient analytics and in-memory processing.


We've chosen the [Iowa Liquor Sales](https://data.iowa.gov/Sales-Distribution/Iowa-Liquor-Sales/m3tr-qhgy/about_data) dataset since it is big enough to make this ETL (extract, transform, load) pipeline interesting.

[^1]: According to their web page :)


## Our problem

Let's say we work for a big chain of liquor stores in the US, Iowa. Part of the intelligence in your company is built upon the information made available through SODA API, and it feeds some of the dashboards the decision-makers consume. You also use it often to do research and predictive modeling. Some stakeholders have started complaining about the loading times of the dashboards, and you have also been a little frustrated with the time it takes to get the data to train your predictive models.

![](img/predictive_modeling_simpsons.png)

To continue building our situation, let's imagine the year 2020‚Äîwhen the term ‚Äúdata engineer‚Äù was not as popular as it is today. You've just been hired as a data analyst[^2], and according to Google Trends, the term ‚Äúdata engineer‚Äù was only half as popular as it is now. Moreover, a closer look at the trend data from 2020 reveals that most searches for this term originated in tech hubs like California or Washington rather than in states like Iowa.

```{r}
#| label: google_trends_data_eng
#| echo: false
#| warning: false
#| fig-cap: "Google trends for 'Data Engineering' in the US, starting from 2004 to the date (Dec 2024)"

# This dataset comes from google trends, starting in 2004 for all US searches.
data_eng_trends <- read_csv('../data/data_engineer_us_30Dec2024.csv') %>% 
  mutate(Month = paste0(Month, '-01'),
         Month = as.Date(Month))

# Chart
ggplot(data_eng_trends, aes(x = Month, y = `data engineer`)) +
  geom_line(color = "#00A8CC", size = 1) +
    geom_vline(xintercept = as.Date("2020-01-01"), linetype = "dashed", color = "#FF6F61", size = 0.8) +
  labs(
    title = "Trends for 'Data Engineer' in google searches within the US",
    x = "Point in time",
    y = "Trend (normalized) value"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    axis.title = element_text(size = 12)
  )
```

At this point, you know you are on your own to optimize this process[^3]. And you already have a clear outline for this process:

1. Pull the data from the API.
2. Generate the variables that provide the most valuable insights for the teamÔ∏è.
3. Store the data in a location that allows for easy and fast retrieval .

This brings us to our current challenge: finding a more agile and efficient way to make the SODA liquor sales data accessible to the rest of the company.

[^2]: There is no budget for a "scientist," and at this point in history, there is no such thing as a "data engineer" in [Latin America]{style="text-decoration: line-through;"} Iowa.
[^3]: Please pretend that the tools existed at the time. I certainly wished for a data ecosystem like this, and I was in Latin America (Little Mai and I still are, and we're loving it ü™áüêë).

## Solution implementation: getting started

As outlined earlier, the first step involves pulling the data from the API. However, before that, we need to set up Mage and configure the rest of our environment. To begin, we'll clone the Git project and navigate to the project directory:

```{bash}
#| label: clone_repo
#| include: true
#| eval: false

git clone https://github.com/jospablo777/mage_duckdb_pipeline.git
cd mage_duckdb_pipeline
```

Next, we'll set up a virtual environment and install the necessary libraries:

```{bash}
#| label: setup_venv
#| include: true
#| eval: false

python -m venv venv             # The first 'venv' is the command, the second is the name of the folder for the virtual environment.
source venv/bin/activate        # Activate the virtual environment.
pip install -r requirements.txt # Install dependencies from the requirements file.
```

The `requirements.txt` file contains the libraries required for the project, with the key players being `mage-ai` and `duckdb`. Now, that we have all our dependencies ready we can start our Mage project with:

```{bash}
#| label: start_mage
#| include: true
#| eval: false

mage start
```

This will open a tab in our browser that looks like this:

![Mage home UI.](img/mage_ui_main.png)

One of Mage's greatest strengths is its intuitive user interface. It allows us to easily create and manage data pipelines. In this example, we've named our pipeline `socrata_iowa_liquor_pipeline`. Once the pipeline is created, we can navigate to it using the left panel, then go to the `Pipelines` section, where our newly created pipeline will be listed, click on it.

![Mage home UI left panel.](img/mage_ui_left_panel.png)

After opening the pipeline, navigate to the Edit pipeline section in the left panel, identified by the </> symbol. This is where we can begin constructing our pipeline. Here, you will have the option to insert a block:

![Create block options.](img/create_block.png)

Our objective is to build the following pipeline using a series of different blocks:

![Pipeline we will implement in this projec.](img/pipeline_dag.png)

Great! Now that our environment is set up and we've familiarized ourselves with Mage's user interface, we'll return to Mage shortly. But first, let's take a closer look at our data source: the [Iowa Liquor Sales](https://data.iowa.gov/Sales-Distribution/Iowa-Liquor-Sales/m3tr-qhgy/about_data) dataset.

## SODA API: our data source

The [Iowa Liquor Sales data](https://data.iowa.gov/Sales-Distribution/Iowa-Liquor-Sales/m3tr-qhgy/about_data) is provided by the Iowa government through the Socrata Open Data API (SODA), a platform designed to grant access to open datasets from government agencies, non-profits, and other organizations. This API allows us to programmatically interact with our dataset of interest, enabling us to retrieve data via HTTP requests.

The [dataset's documentation](https://dev.socrata.com/foundry/data.iowa.gov/m3tr-qhgy) provides the key information needed to access the data: the source domain (`data.iowa.gov`) and the dataset identifier (`m3tr-qhgy`). With this information, we can define the endpoint for sending our requests. The documentation also details the available variables and their respective data types. Below is an example of a base endpoint for this dataset:

```{bash}
#| label: base_soda_endpoint
#| include: true
#| eval: false

https://data.iowa.gov/resource/m3tr-qhgy.csv
```

The dataset documentation also informs us that, to date, it consists of more than 30 million rows. Now that we know the endpoint and the size of the data, we're ready to pull the data, right?

A sensible approach would be to retrieve the data sequentially using the paging method described in the [SODA documentation](https://dev.socrata.com/docs/paging). This method allows us to fetch the data in manageable batches, specified by the `$limit` parameter, while navigating through the dataset using the `$offset` parameter. The process is illustrated in the following diagram:

![Later rows of the data can't be fetched due to system limits.](img/pagination_approach.png)

Naturally, I started with this approach to find a limitation in the API that was not documented. When we get to the point of pulling the records around row 20M, the data loader will get stuck, and no information will be pulled. This might be by design or due to system limitations in which deep paginations can bog the system[^4], making this an unreliable method to get the data[^5]. Meaning that we will need a different strategy to retrieve the 30M records.

This brings us to the next strategy: using [SoQL](https://dev.socrata.com/docs/queries/), the query language of the Socrata API. SoQL is quite similar to SQL, with the key difference that its syntax is structured to work within a URL format.

We will send an HTTP request to the server to query individual batches of invoices corresponding to a specific year (based on the `date` variable). We represent this as follows.

![Fetching each year individually: an incremental approach. The queries in the blocks are written in SQL for readability.](img/soql_approach.png)

This approach limits pagination to the number of years in the dataset rather than the total number of records. By fetching data in yearly batches, our requests won't get stuck at an offset of 20 million, as each year contains fewer than 3 million records.

**Why are we spending so much time on this?** Understanding our data source's quirks, perks, and limitations is crucial because, ultimately, this knowledge will shape how we **design** our data pipeline. So it's worth it to spend some time understanding the source system; it will save us headaches and result in a better design that is easier to maintain from the beginning.

Now, we have a clearer understanding of the upper stage in our stream, the data source.

![Progress we've made so far.](img/soda_done_data_lifecycle_logos.png)

With this in mind, we can now move on to the ingestion step.

[^4]: I spent a few days debugging this and initially signaled Mage as the culprit, but in the end, the SODA API was responsible.
[^5]: The process will get frozen at some point with no notifications or feedback.


## Fetching the data

We are aware of the endpoint to request the data and some limitations of this API. We know that we cannot fetch the data as it is because, at some point, our pipeline will clog, and we won't be able to request more data from the SODA DB.

Due to this, we will write more complex HTTP requests using [SoQL](https://dev.socrata.com/docs/queries/), the SODA query language. SoQL syntax is similar to SQL, so if you are familiar with relational databases, learning will feel intuitive.

Our base query to fetch the data will look something like this:

[https://data.iowa.gov/resource/m3tr-qhgy.csv]{style="color:#008080;"}?[\$where=date_extract_y(date)=2013]{style="color:#4169E1;"}&[$limit=2000]{style="color:#FF8C00;"}

- [**Teal:**]{style="color:#008080;"} the endpoint for the data request.
- [**Blue:**]{style="color:#4169E1;"} the `WHERE` clause of the SoQL query, used to filter records for the year 2013. The function `date_extract_y()` extracts the year from the `date` column.
- [**Orange:**]{style="color:#FF8C00;"} the `LIMIT` clause of the SoQL query, used to limit the response to 2000 records. If we don't indicate this, the default will be used, which is 1000 records.

Notice that the `?` symbol separates the endpoint from the query parameters, and each clause is separated by an `&` symbol. To indicate the start of a clause, a `$` is used.

This request will be sent using the `GET` HTTP method.

With this in mind, we can return to Mage. We'll begin by creating the first three blocks, which will focus on retrieving metadata. The primary purpose of these blocks is to provide the downstream block with the necessary information on what data to fetch from the endpoint and how to fetch it.

:::{.callout-note}
In this tutorial, we will adopt the writing style of [*The Rust Programming Language*](https://doc.rust-lang.org/book/) book, where the file path is shown before each code block. This approach allows you to easily follow the project's structure. The complete code for the project is available on the [GitHub repo](https://github.com/jospablo777/mage_duckdb_pipeline). Also, the function docstrings have been removed from this article. However, you can review the complete technical documentation of each function by checking the files in the repo.
:::

### Wrinting our first Mage blocks

Here we will implement our first block, two of them will fetch metadata from the SODA API, and another one will get metadata from our local database (DuckDB). First lets go to our already created pipeline, `socrata_iowa_liquor_pipeline`. But if you want to follow this tutorial from scratch, you can also create a new pipeline, for example, `socrata_iowa_liquor_pipeline_from_scratch`, and continue working from there. With the intuitive Mage UI, it will be easy for you to create this new pipeline.

Our first block:

![Creating our first block.](img/create_block.png)

Here, select the first option, **Data loader**, then navigate to the creation option and select Python as the language, and then API as the source, then name the block `get_schema_from_metadata`. When created, the block will come with a template like this:

Filename: [`data_loaders/get_schema_from_metadata.py`](https://github.com/jospablo777/mage_duckdb_pipeline/blob/main/data_loaders/get_schema_from_metadata.py)
```{python}
#| label: get_schema_from_metadata_1
#| include: true
#| eval: false

import io # <1>
import pandas as pd
import requests
if 'data_loader' not in globals():
    from mage_ai.data_preparation.decorators import data_loader
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test # <1>


@data_loader # <2>
def load_data_from_api(*args, **kwargs):
    """
    Template for loading data from API
    """
    url = ''
    response = requests.get(url)

    return pd.read_csv(io.StringIO(response.text), sep=',') # <2>


@test # <3>
def test_output(output, *args) -> None:
    """
    Template code for testing the output of the block.
    """
    assert output is not None, 'The output is undefined' # <3>
```
1. **Imports:** block's prelude.
2. **Main functionality:** since this is a data loader I has the `@data_loader` decorator on top.
3. **Test for data validation:** accepts the output data of the block. If any of the tests fail, the block execution will also fail.

Mage provides a variety of templates designed to guide you and save time. These templates were incredibly helpful when we[^6] first started building data pipelines. Notices the structure of a block, it is composed by three sections: library imports, a function, and a test. The function handles the main task of the block, while the test ensures that the block's main functionality works as intended.

Next, we'll modify the template to suit our specific use case. Overwrite the content of your newly created block with the following code, and let's proceed to analyze it:

Filename: [`data_loaders/get_schema_from_metadata.py`](https://github.com/jospablo777/mage_duckdb_pipeline/blob/main/data_loaders/get_schema_from_metadata.py)
```{python}
#| label: get_schema_from_metadata_2
#| include: true
#| eval: false

import io
import polars as pl # <1>
import requests     # <2>
if 'data_loader' not in globals():
    from mage_ai.data_preparation.decorators import data_loader # <3>
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test # <4>

# Map API data types to Polars types
SODA_TO_POLARS = { # <5>
    "text": pl.Utf8,
    "number": pl.Float64, 
    "calendar_date": pl.Datetime("us"),
    "floating_timestamp": pl.Datetime("us"),  
}                  # <5>


# Loads the schema (i.e., types) of our data set
@data_loader # <6>
def load_data_schema_from_api(*args, **kwargs):

    url = 'https://{DOMAIN}/api/views/{DATASET_ID}'.format(**kwargs) # <7>
    response = requests.get(url)

    metadata = response.json()
    columns = metadata.get("columns", [])
    schema = {
        col["fieldName"]: SODA_TO_POLARS.get(col["dataTypeName"], pl.Utf8) # <8>
        for col in columns
        if not col["fieldName"].startswith(":@computed_") # <9>
        }
    
    return schema

@test # <10>
def test_output(dictionary, *args) -> None:
    assert dictionary is not None, "The output is undefined"
    assert isinstance(dictionary, dict), "The output is not a dictionary"
    assert len(dictionary) > 0, "The dictionary is empty" # <10>
```
1. **Polars:** used for data manipulation and type mapping throughout the project.
2. **HTTP library:** `requests` is used to send HTTP requests to the API.
3. **Loader decorator:** the `@data_loader` decorator marks the function as a Mage data loader block.
4. **Test decorator:** the `@test decorator` defines a test for validating the function's output.
5. **Socrata to Polars type mapping:** the `SODA_TO_POLARS` dictionary maps Socrata data types to corresponding Polars types, ensuring compatibility.
6. **Function decoration:** the loader function `load_data_schema_from_api` is decorated with `@data_loader` to integrate it into the Mage pipeline.
7. **Endpoint definition:** the endpoint URL is dynamically generated using the global variables `DOMAIN` and `DATASET_ID`. This is where the schema metadata is fetched.
8. **Dictionary comprehension:** the `schema` dictionary maps column names to their corresponding Polars types, based on the `SODA_TO_POLARS` dictionary. Unrecognized types default to `Utf8`.
9. **Exclusion columns:** columns with names starting with `:@computed_` are excluded from the schema.
10. **Test function:** the `@test` decorator validates the loader's output, ensuring it is a non-empty dictionary of the correct type.

The primary goal of the `get_schema_from_metadata` block is to retrieve data type information to standardize batch ingestion. While Polars can infer data types based on the content they read, each batch may differ in structure. This inconsistency can lead to errors and prevent us from merging batches later. We can resolve this issue by specifying data types and ensuring a consistent schema across all batches.

Please notice that we use Polars for data transformation and manipulation. Compared with Pandas, Polars offers better performance and scalability, making it a more cost-effective choice for production systems. This [case study](https://pola.rs/posts/case-check-technology/) demonstrates that Polars can reduce computational costs, making it a good choice for projects that may eventually transition to production. Polars is a mature and robust library with extensive capabilities, making it an excellent choice for this project.

We can illustrate how the Mage flow operates using our first block. Initially, the block performs its main task‚Äîfetching data from an API in this example. The instructions for this task are defined in the block's primary function, which is identified by the `@data_loader` decorator. Once the main task is completed, its output is passed to the tests, marked with the `@test` decorator. If all tests are successful, the output is forwarded to the next block in the stream.

When you first looked at @fig-concrete-project-outline, you might have wondered why storage spans the entire process and why Apache Arrow and Parquet are included in the storage block alongside DuckDB. This is because data storage underpins every major stage, with data being stored multiple times throughout its life cycle. Mage uses PyArrow to handle data serialization and deserialization between blocks during these storage steps[^7]. For disk storage, PyArrow serializes the data into the Parquet format. For more details, you can check the the [documentation](https://docs.mage.ai/guides/blocks/polars).

Below is an overview of how Mage blocks function:

![Overview of Mage blocks functioning.](img/mage_blocks_function.png)

Congrats! You just implemented your first Mage block. Let's test it, you can do this with the "play" icon in the header of the block:

![](img/testing_mage_block.png)

After the block runs, you should be able to see the output in the tail of the block:

![](img/testing_mage_block_2.png)

We see a message indicating that the test passed and how Mage tries to display the output dictionary.

Next, we will query the number of records (invoices) per year. To achieve this, we will create a data loader block named `soda_records_per_year.` The content of this block will look as follows:

Filename: [`data_loaders/soda_records_per_year.py`](https://github.com/jospablo777/mage_duckdb_pipeline/blob/main/data_loaders/soda_records_per_year.py)
```{python}
#| label: soda_records_per_year
#| include: true
#| eval: false

import io # <1>
import polars as pl
import requests
if 'data_loader' not in globals():
    from mage_ai.data_preparation.decorators import data_loader
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test


@data_loader # <2>
def load_data_from_api(*args, **kwargs):
    # SoQL to get the the number of invoices per year
    data_url = "https://data.iowa.gov/resource/m3tr-qhgy.csv?$select=date_extract_y(date) AS year, count(invoice_line_no) AS rows&$group=date_extract_y(date)"
    print("\n")
    print("Fetching the records-per-year metadata. This might take a couple minutes..")
    response = requests.get(data_url)
    print("Done! We have our year record metadata.\n")
    data = pl.read_csv(io.StringIO(response.text))
    
    return data # <2>


@test # <3>
def test_output(output, *args) -> None:
    assert output is not None, 'The output is undefined'
    assert isinstance(output, pl.DataFrame), 'The output is not a Polars DataFrame' # <3>

```
1. You might have noticed that we use the `io` module to handle the API responses. Specifically, we utilize the `StringIO()` class to treat the response as a document. Otherwise, Polars will complain. 
2. Here, we make a SoQL query to the API, and load the response into a Polars data frame.
3. We also validate that the block's output is not empty and confirm that it is a Polars data frame.

In this block we request the records per year to the API, for this we make use of the SoQL language provided by the API developers:

[https://data.iowa.gov/resource/m3tr-qhgy.csv]{style="color:#008080;"}?[\$select=date_extract_y(date) AS year, count(invoice_line_no) AS rows&\$group=date_extract_y(date)]{style="color:#4169E1;"}

And again, we can dissect this request:

- [**Teal:**]{style="color:#008080;"} endpoint.
- [**Blue:**]{style="color:#4169E1;"} query.

If we translate this query to SQL, we would have something like this:

```{sql}
#| label: soda_records_per_year_to_sql
#| include: true
#| eval: false

SELECT YEAR(date) AS year
       COUNT(invoice_line_no) AS rows
FROM m3tr-qhgy -- Iowa Liquor Sales table
GROUP BY YEAR(date)
```

The output should be a table displaying the number of records for each year. This information will be used to determine the `$limit` clause when retrieving data later.

To complete the upstream blocks, we will create another block to check the data available in our local DuckDB database. I named this block `check_local_db`. It is of type "custom," though it could also have been implemented as a data loader.

Filename: [`custom/check_local_db.py`](https://github.com/jospablo777/mage_duckdb_pipeline/blob/main/custom/check_local_db.py)
```{python}
#| label: check_local_db
#| include: true
#| eval: false
# -- snip --

db_path = 'data/iowa_liquor.duckdb' # <1>

@custom
def check_last_year(*args, **kwargs):
    conn = duckdb.connect("data/iowa_liquor.duckdb") # <2>
    
    try: # <3>
        result = conn.execute("""
        SELECT EXTRACT(YEAR FROM MAX(date)) FROM iowa_liquor_sales
        """).fetchall()
        last_year = result[0][0] # <3>

    except duckdb.CatalogException as e: # <4>
        print("The table doesn't exist; assigning rows_in_db=0")
        last_year = 0 # <4>

    except Exception as e: # <5>
        print(f"An unexpected error occurred: {e}")
        last_year = 0 # <5>

    # Close DB connections
    conn.close() # <6>

    return last_year # <7>

# -- snip --
```
1. Define the path to the database. For this project, the database will be stored in the file `data/iowa_liquor.duckdb`.
2. Establish a connection to the local database. If the file `iowa_liquor.duckdb` does not exist, DuckDB will automatically create it.
3. Query the date of the most recent invoice in the database and extract the year. Use `.fetchall()` to retrieve the query output as a Python object[^8].
4. If the table does not yet exist in the database, return `0` as the year.
5. If any other exception occurs, also return `0` as the year.
6. Explicitly close the database connection to avoid blocking access for downstream processes.
7. Return the most recent year as an `int`.

This block is designed to monitor the local database, enabling efficient updates by avoiding the need to fetch all data every time. This approach ensures faster processing and reduces resource consumption.

With that, we've completed the first layer of Mage blocks. The next step downstream involves retrieving the data of interest. In this stage, we will use the outputs of the three blocks we just created

[^6]: Little Mai and I üêë.
[^7]: This occurs when the returned object is a data frame (Polars or Pandas). In this concrete case, that won't happen since we're returning a dictionary, but in the following blocks, it will be the case since the objects transferred will be Polars data frames. With dictionaries, serialization will still occur but in [JSON format](https://github.com/mage-ai/mage-ai/issues/4725).
[^8]: We can also use `.pl()` to cast the results into a Polars data frame or `.df()` for a Pandas one.

### Pulling the Iowa Liquor Sales data

With the metadata at hand now we will be able to get the data of interest (the sales data)



`get_schema_from_metadata`

`soda_records_per_year`

`check_local_db`

Bash execution

```{bash}
#| label: bash_pipeline_output
#| include: true
#| eval: false

user@user-pc:~/mage_duckdb_pipeline$ mage run . socrata_iowa_liquor_pipeline
Fetching the records-per-year metadata. This might take a couple minutes..
Done! We have our year record metadata.

SODA data pull started.
Years to be fetched: 2017, 2018, 2019, 2020, 2021.
Fetching data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [10:08<00:00, 121.69s/it]
Product-related new variables, generated.
Sales and price related metrics, computed.
Volume-based features, computed.
Time-based features, computed.
Data loaded to your DuckDB database!
Pipeline run completed.
```


```{r}
#| label: dbi_connection
conn2 <- DBI::dbConnect(
  drv = duckdb::duckdb(),
  dbdir = "../data/iowa_liquor.duckdb",
  read_only = TRUE
)
```

```{sql}
#| label: example_query
#| output.var: query2
#| echo: true
#| output: true
#| connection: conn2

SELECT *
FROM iowa_liquor_sales
LIMIT 10
```
```{r}
tibble::tibble(query2)
```

```{python}
#| label: python_conn
import duckdb

con_py = duckdb.connect("../data/iowa_liquor.duckdb", read_only=True)

polars_df = con_py.sql("SELECT liquor_type, bottle_size, price_per_liter FROM iowa_liquor_sales LIMIT 10").pl()

polars_df
```
```{python}
polars_df.height
```

## About the polar bear in the header

https://knowyourmeme.com/memes/bonjour-bear

## Cited works and recommended readings

- Reis, J., & Housley, M. (2022). *Fundamentals of data engineering: Plan and build robust data systems*. O'Reilly Media.