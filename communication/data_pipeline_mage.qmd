---
title: "Building a data pipeline with Mage, Polars, and DuckDB"
include-in-header:
  - text: |
      <link rel = "shortcut icon" href = "img/favicon.png" />
author:
  name: "José P. Barrantes"
  url: "https://www.linkedin.com/in/jose-barrantes/"
description: "GitHub Repo: [SODA to DuckDB](https://github.com/jospablo777/)"
date: "`r Sys.Date()`"
published-title: "Last updated"
format: 
  html:
    toc: true
    toc-location: right
    toc-title: "On this page"
    aside: true
editor: source
---

![](img/bonjour_bear.png)

## Where are we in the data lifecycle?

As data analysts or scientists, we often find ourselves working downstream in the data lifecycle. Most of the time, our role involves transforming and analyzing data that has already been prepared and served to us by upstream processes. However, having a deeper understanding of the entire data pipeline—from ingestion to transformation and storage—can empower us to optimize workflows, ensure data quality, and unlock new insights. 

![The data lifecycle and where you are. Modified from "Fundamentals of Data Engineering" by Reis & Housley (2022).](img/data_lifecycle.png)

Another advantage of gaining understanding—and hands-on experience—in data engineering processes is the **empathy** we build with our data engineers. These are the colleagues we work closely with and rely on, making a strong, **collaborative relationship** essential. Hence, in this hands-on article, we’ll dive into the Python ecosystem, exploring tools like Mage, Polars, and DuckDB to demonstrate how they can help us construct efficient, lightweight data pipelines that bridge the gap between upstream and downstream processes.

## About the tools

In this project we will use

### What are data orchestrators?

We will use Mage as our orchestrator

### Data manipulation and storage

But why Polars and DuckDB? well... we will be dealing with a small (enough) dataset which can be easily handled in memory. Here is not need for a distributed system




![The data lifecycle and a more concrete overview of what **we will implement** in this project. Modified from "Fundamentals of Data Engineering" by Reis & Housley (2022).](img/data_lifecycle_logos.png)



Using only the Python ecosystem.

![](img/pipeline_dag.png)

```{r}
#| label: setup
#| include: false
#| output: false
#| eval: true

# Tell quarto where is our python venv
Sys.setenv(RETICULATE_PYTHON = "../venv/bin/python")
```

Token

```{python}
#| label: token_mention
#| include: true
#| eval: false

headers = {"X-App-Token": "YOUR_TOKEN"}
response = requests.get(data_url, headers=headers)
```

Bash execution

```{bash}
#| label: bash_pipeline_output
#| include: true
#| eval: false

user@user-pc:~/mage_duckdb_pipeline$ mage run . socrata_iowa_liquor_pipeline
Fetching the records-per-year metadata. This might take a couple minutes..
Done! We have our year record metadata.

SODA data pull started.
Years to be fetched: 2017, 2018, 2019, 2020, 2021.
Fetching data: 100%|█████████████████████████████████████| 5/5 [10:08<00:00, 121.69s/it]
Product-related new variables, generated.
Sales and price related metrics, computed.
Volume-based features, computed.
Time-based features, computed.
Data loaded to your DuckDB database!
Pipeline run completed.
```


```{r}
conn2 <- DBI::dbConnect(
  drv = duckdb::duckdb(),
  dbdir = "../data/iowa_liquor.duckdb",
  read_only = TRUE
)
```

```{sql}
#| output.var: query2
#| echo: true
#| output: true
#| connection: conn2

SELECT *
FROM iowa_liquor_sales
LIMIT 10
```
```{r}
tibble::tibble(query2)
```

```{python}
import duckdb

con_py = duckdb.connect("../data/iowa_liquor.duckdb", read_only=True)

polars_df = con_py.sql("SELECT liquor_type, bottle_size, price_per_liter FROM iowa_liquor_sales LIMIT 10").pl()

polars_df
```
```{python}
polars_df.height
```

## About the polar bear in the header

https://knowyourmeme.com/memes/bonjour-bear

## Cited works and recommended readings

- Reis, J., & Housley, M. (2022). *Fundamentals of data engineering: Plan and build robust data systems*. O'Reilly Media.