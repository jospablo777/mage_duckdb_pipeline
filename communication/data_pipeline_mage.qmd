---
title: "Building a data pipeline with Mage, Polars, and DuckDB"
include-in-header:
  - text: |
      <link rel = "shortcut icon" href = "img/favicon.png" />
author:
  name: "Jos√© P. Barrantes"
  url: "https://www.linkedin.com/in/jose-barrantes/"
description: "GitHub Repo: [SODA to DuckDB](https://github.com/jospablo777/)"
date: "`r Sys.Date()`"
published-title: "Last updated"
format: 
  html:
    toc: true
    toc-location: right
    toc-title: "On this page"
    aside: true
editor: source
---

```{r}
#| label: setup
#| include: false
#| output: false
#| eval: true

# Prelude libraries
library(readr)
library(dplyr)
library(ggplot2)

# Tell quarto where is our python venv
Sys.setenv(RETICULATE_PYTHON = "../venv/bin/python")
```

![](img/bonjour_bear.png)

## Where are we in the data lifecycle?

As data analysts or scientists, we often find ourselves working downstream in the data lifecycle. Most of the time, our role involves transforming and analyzing data that has already been prepared and served to us by upstream processes. However, having a deeper understanding of the entire data pipeline‚Äîfrom ingestion to transformation and storage‚Äîcan empower us to optimize workflows, ensure data quality, and unlock new insights. 

![The data lifecycle and where you are. Modified from "Fundamentals of Data Engineering" by Reis & Housley (2022).](img/data_lifecycle.png)

Another advantage of gaining understanding‚Äîand hands-on experience‚Äîin data engineering processes is the **empathy** we build with our data engineers. These are the colleagues we work closely with and rely on, making a strong, **collaborative relationship** essential. Hence, in this hands-on article, we will explore the Python ecosystem by examining tools such as Mage, Polars, and DuckDB. We'll demonstrate how these tools can help us build efficient, lightweight data pipelines that take data from the source and store it in a format that is well-suited for high-performance analytics.

## About the tools

In this project, we will use Mage as our data orchestrator. The tasks managed by the orchestrator will include data manipulation and transformation using Polars, as well as persistent storage with DuckDB.

### What are data orchestrators?

Data orchestrators, such as Mage, are tools that help manage, schedule, and monitor workflows in data pipelines. They allow us to automate complex processes, ensuring that tasks are executed in the correct order and dependencies are handled seamlessly. By using Mage as our orchestrator, we can streamline our data pipeline and focus on building efficient workflows that allow us to set a local database appropriate for our analysis.

### Data manipulation and storage

Why Polars and DuckDB? The answer lies in the size and nature of the dataset we're working with. Since our dataset is small enough to fit in memory, we don't need a distributed system like Spark. Polars, with its fast and memory-efficient operations, is perfect for data manipulation and transformation. Meanwhile, DuckDB provides a lightweight, yet powerful, SQL-based engine for persistent storage and querying. Together, these tools offer a simple, performant, and highly efficient solution for handling our data pipeline.

Returning to the data lifecycle diagram, we can land it in a more concrete way to show how our project will be built and executed:

![The data lifecycle and a more concrete overview of what **we will implement** in this project. Modified from "Fundamentals of Data Engineering" by Reis & Housley (2022).](img/data_lifecycle_logos.png)

First, we will fetch the data from the Socrata Open Data API (SODA) through HTTP. After that, we will generate some extra variables of our interest with Polars, to finally store it in a DuckDB database we will consume for analytics and predictive modeling. All this is orchestrated with Mage.

### Tools and resources overview:

- [**SODA API:**](https://dev.socrata.com/) provides access to open datasets, serving as our data source. It contains "a wealth of open data resources from governments, non-profits, and NGOs around the world[^1]."
- [**Mage:**](https://www.mage.ai/) the data orchestrator that will automate and manage the pipeline.
- [**Polars:**](https://pola.rs/) a high-performance data frame library implemented in Rust, ideal for fast data manipulation.
- [**DuckDB:**](https://duckdb.org/) a columnar database system designed for efficient analytics and in-memory processing.


We've chosen the [Iowa Liquor Sales](https://data.iowa.gov/Sales-Distribution/Iowa-Liquor-Sales/m3tr-qhgy/about_data) dataset since it is big enough to make this (extract, transform, load) ETL process interesting.

[^1]: According to their web page :)


## Our problem

Let's say we work for a big chain of liquor stores in the US, Iowa. Part of the intelligence in your company is built upon the information made available through SODA API, and it feeds some of the dashboards the decision-makers consume. You also use it often to do research and predictive modeling. Some stakeholders have started complaining about the loading times of the dashboards, and you have also been a little frustrated with the time it takes to get the data to train your predictive models.

![](img/predictive_modeling_simpsons.png)

To continue building our situation, let's imagine the year 2020‚Äîwhen the term ‚Äúdata engineer‚Äù was not as popular as it is today. You've just been hired as a data analyst, and according to Google Trends, the term ‚Äúdata engineer‚Äù was only half as popular as it is now. Moreover, a closer look at the trend data from 2020 reveals that most searches for this term originated in tech hubs like California or Washington rather than in states like Iowa.

```{r}
#| label: google_trends_data_eng
#| echo: false
#| warning: false
#| fig-cap: "Google trends for 'Data Engineering' in the US, starting from 2004 to the date (Dec 2024)"

# This dataset comes from google trends, starting in 2004 for all US searches.
data_eng_trends <- read_csv('../data/data_engineer_us_30Dec2024.csv') %>% 
  mutate(Month = paste0(Month, '-01'),
         Month = as.Date(Month))

# Chart
ggplot(data_eng_trends, aes(x = Month, y = `data engineer`)) +
  geom_line(color = "#00A8CC", size = 1) +
    geom_vline(xintercept = as.Date("2020-01-01"), linetype = "dashed", color = "#FF6F61", size = 0.8) +
  labs(
    title = "Trends for 'Data Engineer' in google searches within the US",
    x = "Point in time",
    y = "Trend (normalized) value"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    axis.title = element_text(size = 12)
  )
```

So, at this point you know you're in your on for optimizing this process[^2]. And you already have a clear outline for this process:

1. Pull the data from the API.
2. Generate the variables that provide the most valuable insights for the teamÔ∏è.
3. Store the data in a location that allows for easy and fast retrieval .

This brings us to our current challenge: finding a more agile and efficient way to make the SODA liquor sales data accessible to the rest of the company.

[^2]: Please pretend that the tools existed at the time. I certainly wished for a data ecosystem like this, and I was in Latin America (Little Mai and I still are, and we're loving it ü™áüêë).

## Solution implementation: getting started

As per our outline, we know we shall star pulling the data from the API, so...

Token

```{python}
#| label: token_mention
#| include: true
#| eval: false

headers = {"X-App-Token": "YOUR_TOKEN"}
response = requests.get(data_url, headers=headers)
```

Bash execution

```{bash}
#| label: bash_pipeline_output
#| include: true
#| eval: false

user@user-pc:~/mage_duckdb_pipeline$ mage run . socrata_iowa_liquor_pipeline
Fetching the records-per-year metadata. This might take a couple minutes..
Done! We have our year record metadata.

SODA data pull started.
Years to be fetched: 2017, 2018, 2019, 2020, 2021.
Fetching data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [10:08<00:00, 121.69s/it]
Product-related new variables, generated.
Sales and price related metrics, computed.
Volume-based features, computed.
Time-based features, computed.
Data loaded to your DuckDB database!
Pipeline run completed.
```


```{r}
conn2 <- DBI::dbConnect(
  drv = duckdb::duckdb(),
  dbdir = "../data/iowa_liquor.duckdb",
  read_only = TRUE
)
```

```{sql}
#| output.var: query2
#| echo: true
#| output: true
#| connection: conn2

SELECT *
FROM iowa_liquor_sales
LIMIT 10
```
```{r}
tibble::tibble(query2)
```

```{python}
import duckdb

con_py = duckdb.connect("../data/iowa_liquor.duckdb", read_only=True)

polars_df = con_py.sql("SELECT liquor_type, bottle_size, price_per_liter FROM iowa_liquor_sales LIMIT 10").pl()

polars_df
```
```{python}
polars_df.height
```

## About the polar bear in the header

https://knowyourmeme.com/memes/bonjour-bear

## Cited works and recommended readings

- Reis, J., & Housley, M. (2022). *Fundamentals of data engineering: Plan and build robust data systems*. O'Reilly Media.