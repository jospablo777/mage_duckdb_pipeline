[
  {
    "objectID": "data_pipeline_mage.html#where-are-we-in-the-data-lifecycle",
    "href": "data_pipeline_mage.html#where-are-we-in-the-data-lifecycle",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Where are we in the data lifecycle?",
    "text": "Where are we in the data lifecycle?\nAs data analysts or scientists, we often find ourselves working downstream in the data lifecycle. Most of the time, our role involves transforming and analyzing data that has already been prepared and served to us by upstream processes. However, having a deeper understanding of the entire data pipeline—from ingestion to transformation and storage—can empower us to optimize workflows, ensure data quality, and unlock new insights.\n\n\n\nThe data lifecycle and where you are. Modified from “Fundamentals of Data Engineering” by Reis & Housley (2022).\n\n\nAnother advantage of gaining understanding—and hands-on experience—in data engineering processes is the empathy we build with our data engineers. These are the colleagues we work closely with and rely on, making a strong, collaborative relationship essential. Hence, in this hands-on article, we will explore the Python ecosystem by examining tools such as Mage, Polars, and DuckDB. We’ll demonstrate how these tools can help us build efficient, lightweight data pipelines that take data from the source and store it in a format that is well-suited for high-performance analytics."
  },
  {
    "objectID": "data_pipeline_mage.html#about-the-tools",
    "href": "data_pipeline_mage.html#about-the-tools",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "About the tools",
    "text": "About the tools\nIn this project, we will use Mage as our data orchestrator. The tasks managed by the orchestrator will include data manipulation and transformation using Polars, as well as persistent storage with DuckDB.\n\nWhat are data orchestrators?\nData orchestrators, such as Mage, are tools that help manage, schedule, and monitor workflows in data pipelines. They allow us to automate complex processes, ensuring that tasks are executed in the correct order and dependencies are handled seamlessly. By using Mage as our orchestrator, we can streamline our data pipeline and focus on building efficient workflows that allow us to set a local database appropriate for our analysis.\n\n\nData manipulation and storage\nWhy Polars and DuckDB? The answer lies in the size and nature of the dataset we’re working with. Since our dataset is small enough to fit in memory, we don’t need a distributed system like Spark. Polars, with its fast and memory-efficient operations, is perfect for data manipulation and transformation. Meanwhile, DuckDB provides a lightweight, yet powerful, SQL-based engine for persistent storage and querying. Together, these tools offer a simple, performant, and highly efficient solution for handling our data pipeline.\nReturning to the data lifecycle diagram, we can land it in a more concrete way to show how our project will be built and executed:\n\n\n\nThe data lifecycle and a more concrete overview of what we will implement in this project. Modified from “Fundamentals of Data Engineering” by Reis & Housley (2022).\n\n\nFirst, we will fetch the data from the Socrata Open Data API (SODA) through HTTP. After that, we will generate some extra variables of our interest with Polars, to finally store it in a DuckDB database we will consume for analytics and predictive modeling. All this is orchestrated with Mage.\n\n\nTools and resources overview:\n\nSODA API: provides access to open datasets, serving as our data source. It contains “a wealth of open data resources from governments, non-profits, and NGOs around the world1.”\nMage: the data orchestrator that will automate and manage the pipeline.\nPolars: a high-performance data frame library implemented in Rust, ideal for fast data manipulation.\nDuckDB: a columnar database system designed for efficient analytics and in-memory processing.\n\nWe’ve chosen the Iowa Liquor Sales dataset since it is big enough to make this (extract, transform, load) ETL process interesting."
  },
  {
    "objectID": "data_pipeline_mage.html#our-problem",
    "href": "data_pipeline_mage.html#our-problem",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Our problem",
    "text": "Our problem\nLet’s say we work for a big chain of liquor stores in the US, Iowa. Part of the intelligence in your company is built upon the information made available through SODA API, and it feeds some of the dashboards the decision-makers consume. You also use it often to do research and predictive modeling. Some stakeholders have started complaining about the loading times of the dashboards, and you have also been a little frustrated with the time it takes to get the data to train your predictive models.\n\nTo continue building our situation, let’s imagine the year 2020—when the term “data engineer” was not as popular as it is today. You’ve just been hired as a data analyst, and according to Google Trends, the term “data engineer” was only half as popular as it is now. Moreover, a closer look at the trend data from 2020 reveals that most searches for this term originated in tech hubs like California or Washington rather than in states like Iowa."
  },
  {
    "objectID": "data_pipeline_mage.html#solution-implementation-getting-started",
    "href": "data_pipeline_mage.html#solution-implementation-getting-started",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Solution implementation: getting started",
    "text": "Solution implementation: getting started\nAs outlined earlier, the first step involves pulling the data from the API. However, before that, we need to set up Mage and configure the rest of our environment. To begin, we’ll clone the Git project and navigate to the project directory:\n\ngit clone https://github.com/jospablo777/mage_duckdb_pipeline.git\ncd mage_duckdb_pipeline\n\nNext, we’ll set up a virtual environment and install the necessary libraries:\n\npython -m venv venv             # The first 'venv' is the command, the second is the name of the folder for the virtual environment.\nsource venv/bin/activate        # Activate the virtual environment.\npip install -r requirements.txt # Install dependencies from the requirements file.\n\nThe requirements.txt file contains the libraries required for the project, with the key players being mage-ai and duckdb. Now, that we have all our dependencies ready we can start our Mage project with:\n\nmage start"
  },
  {
    "objectID": "data_pipeline_mage.html#about-the-polar-bear-in-the-header",
    "href": "data_pipeline_mage.html#about-the-polar-bear-in-the-header",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "About the polar bear in the header",
    "text": "About the polar bear in the header\nhttps://knowyourmeme.com/memes/bonjour-bear"
  },
  {
    "objectID": "data_pipeline_mage.html#cited-works-and-recommended-readings",
    "href": "data_pipeline_mage.html#cited-works-and-recommended-readings",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Cited works and recommended readings",
    "text": "Cited works and recommended readings\n\nReis, J., & Housley, M. (2022). Fundamentals of data engineering: Plan and build robust data systems. O’Reilly Media."
  },
  {
    "objectID": "data_pipeline_mage.html#footnotes",
    "href": "data_pipeline_mage.html#footnotes",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAccording to their web page :)↩︎"
  }
]