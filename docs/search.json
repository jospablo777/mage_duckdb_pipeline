[
  {
    "objectID": "data_pipeline_mage.html#where-are-we-in-the-data-lifecycle",
    "href": "data_pipeline_mage.html#where-are-we-in-the-data-lifecycle",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Where are we in the data lifecycle?",
    "text": "Where are we in the data lifecycle?\nAs data analysts or scientists, we often find ourselves working downstream in the data lifecycle. Most of the time, our role involves transforming and analyzing data that has already been prepared and served to us by upstream processes. However, having a deeper understanding of the entire data pipeline‚Äîfrom ingestion to transformation and storage‚Äîcan empower us to optimize workflows, ensure data quality, and unlock new insights.\n\n\n\nThe data lifecycle and where you are. Modified from ‚ÄúFundamentals of Data Engineering‚Äù by Reis & Housley (2022).\n\n\nAnother advantage of gaining understanding‚Äîand hands-on experience‚Äîin data engineering processes is the empathy we build with our data engineers. These are the colleagues we work closely with and rely on, making a strong, collaborative relationship essential. Hence, in this hands-on article, we will explore the Python ecosystem by examining tools such as Mage, Polars, and DuckDB. We‚Äôll demonstrate how these tools can help us build efficient, lightweight data pipelines that take data from the source and store it in a format that is well-suited for high-performance analytics."
  },
  {
    "objectID": "data_pipeline_mage.html#about-the-tools",
    "href": "data_pipeline_mage.html#about-the-tools",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "About the tools",
    "text": "About the tools\nIn this project, we will use Mage as our data orchestrator. The tasks managed by the orchestrator will include data manipulation and transformation using Polars, as well as persistent storage with DuckDB.\n\nWhat are data orchestrators?\nData orchestrators, such as Mage, are tools that help manage, schedule, and monitor workflows in data pipelines. They allow us to automate complex processes, ensuring that tasks are executed in the correct order and dependencies are handled seamlessly. By using Mage as our orchestrator, we can streamline our data pipeline and focus on building efficient workflows that allow us to set a local database appropriate for our analysis.\n\n\nData manipulation and storage\nWhy Polars and DuckDB? The answer lies in the size and nature of the dataset we‚Äôre working with. Since our dataset is small enough to fit in memory, we don‚Äôt need a distributed system like Spark. Polars, with its fast and memory-efficient operations, is perfect for data manipulation and transformation. Meanwhile, DuckDB provides a lightweight, yet powerful, SQL-based engine for persistent storage and querying. Together, these tools offer a simple, performant, and highly efficient solution for handling our data pipeline.\nReturning to the data lifecycle diagram, we can land it in a more concrete way to show how our project will be built and executed:\n\n\n\nThe data lifecycle and a more concrete overview of what we will implement in this project. Modified from ‚ÄúFundamentals of Data Engineering‚Äù by Reis & Housley (2022).\n\n\nFirst, we will fetch the data from the Socrata Open Data API (SODA) through HTTP. After that, we will generate some extra variables of our interest with Polars, to finally store it in a DuckDB database we will consume for analytics and predictive modeling. All this is orchestrated with Mage.\n\n\nTools and resources overview:\n\nSODA API: provides access to open datasets, serving as our data source. It contains ‚Äúa wealth of open data resources from governments, non-profits, and NGOs around the world1.‚Äù\nMage: the data orchestrator that will automate and manage the pipeline.\nPolars: a high-performance data frame library implemented in Rust, ideal for fast data manipulation.\nDuckDB: a columnar database system designed for efficient analytics and in-memory processing.\n\nWe‚Äôve chosen the Iowa Liquor Sales dataset since it is big enough to make this (extract, transform, load) ETL process interesting."
  },
  {
    "objectID": "data_pipeline_mage.html#our-problem",
    "href": "data_pipeline_mage.html#our-problem",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Our problem",
    "text": "Our problem\nLet‚Äôs say we work for a big chain of liquor stores in the US, Iowa. Part of the intelligence in your company is built upon the information made available through SODA API, and it feeds some of the dashboards the decision-makers consume. You also use it often to do research and predictive modeling. Some stakeholders have started complaining about the loading times of the dashboards, and you have also been a little frustrated with the time it takes to get the data to train your predictive models.\n\nTo continue building our situation, let‚Äôs imagine the year 2020‚Äîwhen the term ‚Äúdata engineer‚Äù was not as popular as it is today. You‚Äôve just been hired as a data analyst, and according to Google Trends, the term ‚Äúdata engineer‚Äù was only half as popular as it is now. Moreover, a closer look at the trend data from 2020 reveals that most searches for this term originated in tech hubs like California or Washington rather than in states like Iowa.\n\n\n\n\n\nGoogle trends for ‚ÄòData Engineering‚Äô in the US, starting from 2004 to the date (Dec 2024)\n\n\n\n\nSo, at this point you know you‚Äôre in your on for optimizing this process2. And you already have a clear outline for this process:\n\nPull the data from the API.\nGenerate the variables that provide the most valuable insights for the teamÔ∏è.\nStore the data in a location that allows for easy and fast retrieval .\n\nThis brings us to our current challenge: finding a more agile and efficient way to make the SODA liquor sales data accessible to the rest of the company."
  },
  {
    "objectID": "data_pipeline_mage.html#solution-implementation-getting-started",
    "href": "data_pipeline_mage.html#solution-implementation-getting-started",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Solution implementation: getting started",
    "text": "Solution implementation: getting started\nAs outlined earlier, the first step involves pulling the data from the API. However, before that, we need to set up Mage and configure the rest of our environment. To begin, we‚Äôll clone the Git project and navigate to the project directory:\n\ngit clone https://github.com/jospablo777/mage_duckdb_pipeline.git\ncd mage_duckdb_pipeline\n\nNext, we‚Äôll set up a virtual environment and install the necessary libraries:\n\npython -m venv venv             # The first 'venv' is the command, the second is the name of the folder for the virtual environment.\nsource venv/bin/activate        # Activate the virtual environment.\npip install -r requirements.txt # Install dependencies from the requirements file.\n\nThe requirements.txt file contains the libraries required for the project, with the key players being mage-ai and duckdb. Now, that we have all our dependencies ready we can start our Mage project with:\n\nmage start\n\nThis will open a tab in our browser that looks like this:\n\n\n\nMage home UI.\n\n\nOne of the strongest of Mage is its easy user interface. Here we will be able to create our data pipelines, I named the one in our example socrata_iowa_liquor_pipeline. After creating the pipeline, we can navigate to it with the left panel, and go to the Pipelines section, there it should be the pipeline we just created.\n\n\n\nMage home UI left panel\n\n\nWhen we open it, also in the left panes, we can go to the Edit pipeline section and start building\nToken\n\nheaders = {\"X-App-Token\": \"YOUR_TOKEN\"}\nresponse = requests.get(data_url, headers=headers)\n\nBash execution\n\nuser@user-pc:~/mage_duckdb_pipeline$ mage run . socrata_iowa_liquor_pipeline\nFetching the records-per-year metadata. This might take a couple minutes..\nDone! We have our year record metadata.\n\nSODA data pull started.\nYears to be fetched: 2017, 2018, 2019, 2020, 2021.\nFetching data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [10:08&lt;00:00, 121.69s/it]\nProduct-related new variables, generated.\nSales and price related metrics, computed.\nVolume-based features, computed.\nTime-based features, computed.\nData loaded to your DuckDB database!\nPipeline run completed.\n\n\nconn2 &lt;- DBI::dbConnect(\n  drv = duckdb::duckdb(),\n  dbdir = \"../data/iowa_liquor.duckdb\",\n  read_only = TRUE\n)\n\n\nSELECT *\nFROM iowa_liquor_sales\nLIMIT 10\n\n\ntibble::tibble(query2)\n\n# A tibble: 10 √ó 36\n   invoice_line_no date                store name          address city  zipcode\n   &lt;chr&gt;           &lt;dttm&gt;              &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  \n 1 S10440300006    2013-02-05 00:00:00 2655  HY-VEE FOOD ‚Ä¶ 1200 S‚Ä¶ CLAR‚Ä¶ 51632  \n 2 S16464700176    2013-12-23 00:00:00 2638  HY-VEE WINE ‚Ä¶ 5925 U‚Ä¶ CEDA‚Ä¶ 50613  \n 3 S12001000067    2013-05-02 00:00:00 2561  HY-VEE FOOD ‚Ä¶ 4605 F‚Ä¶ DES ‚Ä¶ 50321  \n 4 S12643000007    2013-06-06 00:00:00 3973  MMDG SPIRITS‚Ä¶ 126A W‚Ä¶ AMES  50014  \n 5 S14029600009    2013-08-21 00:00:00 3776  WAL-MART 511‚Ä¶ 3101 W‚Ä¶ DAVE‚Ä¶ 52806  \n 6 S15244000066    2013-10-21 00:00:00 2190  CENTRAL CITY‚Ä¶ 1460 2‚Ä¶ DES ‚Ä¶ 50314  \n 7 S10238500019    2013-01-24 00:00:00 4023  WAL-MART 138‚Ä¶ 1515 S‚Ä¶ BOONE 50036  \n 8 S13319200017    2013-07-11 00:00:00 4426  LIQUOR AND G‚Ä¶ 114 CE‚Ä¶ MARS‚Ä¶ 50158  \n 9 S14901500074    2013-10-02 00:00:00 4129  CYCLONE LIQU‚Ä¶ 626 LI‚Ä¶ AMES  50010  \n10 S15397800035    2013-10-29 00:00:00 2565  HY-VEE FOOD ‚Ä¶ 819 N ‚Ä¶ SPEN‚Ä¶ 51301  \n# ‚Ñπ 29 more variables: store_location &lt;chr&gt;, county_number &lt;chr&gt;, county &lt;chr&gt;,\n#   category &lt;chr&gt;, category_name &lt;chr&gt;, vendor_no &lt;chr&gt;, vendor_name &lt;chr&gt;,\n#   itemno &lt;chr&gt;, im_desc &lt;chr&gt;, pack &lt;dbl&gt;, bottle_volume_ml &lt;dbl&gt;,\n#   state_bottle_cost &lt;dbl&gt;, state_bottle_retail &lt;dbl&gt;, sale_bottles &lt;dbl&gt;,\n#   sale_dollars &lt;dbl&gt;, sale_liters &lt;dbl&gt;, sale_gallons &lt;dbl&gt;,\n#   liquor_type &lt;chr&gt;, is_premium &lt;lgl&gt;, bottle_size &lt;chr&gt;,\n#   gov_profit_margin &lt;dbl&gt;, gov_retail_markup_percentage &lt;dbl&gt;, ‚Ä¶\n\n\n\nimport duckdb\n\ncon_py = duckdb.connect(\"../data/iowa_liquor.duckdb\", read_only=True)\n\npolars_df = con_py.sql(\"SELECT liquor_type, bottle_size, price_per_liter FROM iowa_liquor_sales LIMIT 10\").pl()\n\npolars_df\n\n\nshape: (10, 3)\n\n\n\nliquor_type\nbottle_size\nprice_per_liter\n\n\nstr\nstr\nf32\n\n\n\n\n\"Whisky\"\n\"large\"\n16.651428\n\n\n\"Whisky\"\n\"medium\"\n29.92\n\n\n\"Cream\"\n\"medium\"\n25.0\n\n\n\"Rum\"\n\"large\"\n10.91\n\n\n\"Whisky\"\n\"medium\"\n42.0\n\n\n\"Whisky\"\n\"large\"\n11.994286\n\n\n\"Rum\"\n\"large\"\n6.497143\n\n\n\"Whisky\"\n\"medium\"\n25.879999\n\n\n\"Gin\"\n\"medium\"\n10.013333\n\n\n\"Vodka\"\n\"medium\"\n26.24\n\n\n\n\n\n\n\npolars_df.height\n\n10"
  },
  {
    "objectID": "data_pipeline_mage.html#about-the-polar-bear-in-the-header",
    "href": "data_pipeline_mage.html#about-the-polar-bear-in-the-header",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "About the polar bear in the header",
    "text": "About the polar bear in the header\nhttps://knowyourmeme.com/memes/bonjour-bear"
  },
  {
    "objectID": "data_pipeline_mage.html#cited-works-and-recommended-readings",
    "href": "data_pipeline_mage.html#cited-works-and-recommended-readings",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Cited works and recommended readings",
    "text": "Cited works and recommended readings\n\nReis, J., & Housley, M. (2022). Fundamentals of data engineering: Plan and build robust data systems. O‚ÄôReilly Media."
  },
  {
    "objectID": "data_pipeline_mage.html#footnotes",
    "href": "data_pipeline_mage.html#footnotes",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAccording to their web page :)‚Ü©Ô∏é\nPlease pretend that the tools existed at the time. I certainly wished for a data ecosystem like this, and I was in Latin America (Little Mai and I still are, and we‚Äôre loving it ü™áüêë).‚Ü©Ô∏é"
  }
]