[
  {
    "objectID": "data_pipeline_mage.html#where-are-we-in-the-data-lifecycle",
    "href": "data_pipeline_mage.html#where-are-we-in-the-data-lifecycle",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Where are we in the data lifecycle?",
    "text": "Where are we in the data lifecycle?\nAs data analysts or scientists, we often find ourselves working downstream in the data lifecycle. Most of the time, our role involves transforming and analyzing data that has already been prepared and served to us by upstream processes. However, having a deeper understanding of the entire data pipeline‚Äîfrom ingestion to transformation and storage‚Äîcan empower us to optimize workflows, ensure data quality, and unlock new insights.\n\n\n\nThe data lifecycle and where you are. Modified from ‚ÄúFundamentals of Data Engineering‚Äù by Reis & Housley (2022).\n\n\nAnother advantage of gaining understanding‚Äîand hands-on experience‚Äîin data engineering processes is the empathy we build with our data engineers. These are the colleagues we work closely with and rely on, making a strong, collaborative relationship essential. Hence, in this hands-on article, we will explore the Python ecosystem by examining tools such as Mage, Polars, and DuckDB. We‚Äôll demonstrate how these tools can help us build efficient, lightweight data pipelines that take data from the source and store it in a format that is well-suited for high-performance analytics."
  },
  {
    "objectID": "data_pipeline_mage.html#about-the-tools",
    "href": "data_pipeline_mage.html#about-the-tools",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "About the tools",
    "text": "About the tools\nIn this project, we will use Mage as our data orchestrator. The tasks the orchestrator manages will include data fetching, manipulation and transformation using Polars, and persistent storage with DuckDB.\n\nWhat are data orchestrators?\nData orchestrators, such as Mage, are tools that help manage, schedule, and monitor workflows in data pipelines. They allow us to automate complex processes, ensuring that tasks are executed in the correct order and dependencies are handled seamlessly. By using Mage as our orchestrator, we can streamline our data pipeline and focus on building efficient workflows that allow us to set a local database appropriate for our analysis.\n\n\nData manipulation and storage\nWhy Polars and DuckDB? The answer lies in the size and nature of the dataset we‚Äôre working with. Since our dataset is small enough to fit in memory, we don‚Äôt need a distributed system like Spark. Polars, with its fast and memory-efficient operations, is perfect for data manipulation and transformation. Meanwhile, DuckDB provides a lightweight, yet powerful, SQL-based engine for persistent storage and querying. Together, these tools offer a simple, performant, and highly efficient solution for handling our data pipeline.\nReturning to the data lifecycle diagram, we can land it in a more concrete way to show how our project will be built and executed:\n\n\n\n\n\n\nNamed figure¬†1: The data lifecycle and a more concrete overview of what we will implement in this project. Modified from ‚ÄúFundamentals of Data Engineering‚Äù by Reis & Housley (2022).\n\n\n\nFirst, we will fetch the data from the Socrata Open Data API (SODA) through HTTP. After that, we will generate some extra variables of our interest with Polars, to finally store it in a DuckDB database. We will consume this database for analytics and predictive modeling. All this is orchestrated with Mage.\n\n\nTools and resources overview:\n\nSODA API: provides access to open datasets, serving as our data source. It contains ‚Äúa wealth of open data resources from governments, non-profits, and NGOs around the world1.‚Äù\nMage: the data orchestrator that will automate and manage the pipeline.\nPolars: a high-performance data frame library implemented in Rust, ideal for fast data manipulation.\nDuckDB: a columnar database system designed for efficient analytics and in-memory processing.\n\nWe‚Äôve chosen the Iowa Liquor Sales dataset since it is big enough to make this ETL (extract, transform, load) pipeline interesting."
  },
  {
    "objectID": "data_pipeline_mage.html#our-problem",
    "href": "data_pipeline_mage.html#our-problem",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Our problem",
    "text": "Our problem\nLet‚Äôs say we work for a big chain of liquor stores located in the US, Iowa. Part of the intelligence in your company is built upon the information made available through SODA API, and it feeds some of the dashboards the decision-makers consume. You also use it often to do research and predictive modeling. Some stakeholders have started complaining about the loading times of the dashboards, and you have also been a little frustrated with the time it takes to get the data to train your predictive models.\n\nTo continue building our situation, let‚Äôs imagine the year 2020‚Äîwhen the term ‚Äúdata engineer‚Äù was not as popular as it is today. You‚Äôve just been hired as a data analyst2, and according to Google Trends, the term ‚Äúdata engineer‚Äù was only half as popular as it is now. Moreover, a closer look at the trend data from 2020 reveals that most searches for this term originated in tech hubs like California or Washington rather than in states like Iowa.\n\n\n\n\n\nGoogle trends for ‚ÄòData Engineering‚Äô in the US, starting from 2004 to the date (Dec 2024)\n\n\n\n\nAt this point, you know you are on your own to optimize this process3. And you already have a clear outline for this process:\n\nPull the data from the API.\nGenerate the variables that provide the most valuable insights for the teamÔ∏è.\nStore the data in a location that allows for easy and fast retrieval .\n\nThis brings us to our current challenge: finding a more agile and efficient way to make the SODA liquor sales data accessible to the rest of the company."
  },
  {
    "objectID": "data_pipeline_mage.html#solution-implementation-getting-started",
    "href": "data_pipeline_mage.html#solution-implementation-getting-started",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Solution implementation: getting started",
    "text": "Solution implementation: getting started\nAs outlined earlier, the first step involves pulling the data from the API. However, before that, we need to set up Mage and configure the rest of our environment. To begin, we‚Äôll clone the Git project and navigate to the project directory:\n\ngit clone https://github.com/jospablo777/mage_duckdb_pipeline.git\ncd mage_duckdb_pipeline\n\nNext, we‚Äôll set up a virtual environment and install the necessary libraries:\n\npython -m venv venv             # The first 'venv' is the command, the second is the name of the folder for the virtual environment.\nsource venv/bin/activate        # Activate the virtual environment.\npip install -r requirements.txt # Install dependencies from the requirements file.\n\nThe requirements.txt file contains the libraries required for the project, with the key players being mage-ai, polars, and duckdb. Now, that we have all our dependencies ready we can start our Mage project with:\n\nmage start\n\nThis will open a tab in our web browser that looks like this:\n\n\n\nMage home UI.\n\n\nOne of Mage‚Äôs greatest strengths is its intuitive user interface. It allows us to easily create and manage data pipelines. In this example, we‚Äôve named our pipeline socrata_iowa_liquor_pipeline. Once the pipeline is created, we can navigate to it using the left panel, then go to the Pipelines section, where our newly created pipeline will be listed, click on it.\n\n\n\nMage home UI left panel.\n\n\nAfter opening the pipeline, navigate to the Edit pipeline section in the left panel, identified by the &lt;/&gt; symbol. This is where we can begin constructing our pipeline. Here, you will have the option to insert a block:\n\n\n\nCreate block options.\n\n\nOur objective is to build the following pipeline using a series of different blocks:\n\n\n\nPipeline we will implement in this projec.\n\n\nGreat! Now that our environment is set up and we‚Äôve familiarized ourselves with Mage‚Äôs user interface, let‚Äôs take a closer look at our data source: the Iowa Liquor Sales dataset. We‚Äôll return to Mage shortly."
  },
  {
    "objectID": "data_pipeline_mage.html#about-the-polar-bear-in-the-header",
    "href": "data_pipeline_mage.html#about-the-polar-bear-in-the-header",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "About the polar bear in the header",
    "text": "About the polar bear in the header\nIt‚Äôs the Bonjour Bear, you can read more about the bear in Know Your Meme :)"
  },
  {
    "objectID": "data_pipeline_mage.html#cited-works-and-recommended-readings",
    "href": "data_pipeline_mage.html#cited-works-and-recommended-readings",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Cited works and recommended readings",
    "text": "Cited works and recommended readings\n\nKersten, T., Leis, V., Kemper, A., Neumann, T., Pavlo, A., & Boncz, P. (2018). Everything you always wanted to know about compiled and vectorized queries but were afraid to ask. Proceedings of the VLDB Endowment, 11(13), 2209-2222. https://doi.org/10.14778/3275366.3275370.\nLovelace, R., Nowosad, J., & Muenchow, J. (2025). Geocomputation with R (2nd ed.). Chapman and Hall/CRC. https://r.geocompx.org/.\nReis, J., & Housley, M. (2022). Fundamentals of data engineering: Plan and build robust data systems. O‚ÄôReilly Media."
  },
  {
    "objectID": "data_pipeline_mage.html#footnotes",
    "href": "data_pipeline_mage.html#footnotes",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAccording to their web page :)‚Ü©Ô∏é\nThere is no budget for a ‚Äúscientist,‚Äù and at this point in history, there is no such thing as a ‚Äúdata engineer‚Äù in Latin America Iowa.‚Ü©Ô∏é\nPlease pretend that the tools existed at the time. I certainly wished for a data ecosystem like this, and I was in Latin America (Little Mai and I still are, and we‚Äôre loving it ü™áüêë).‚Ü©Ô∏é\nI spent a few days debugging this and initially signaled Mage as the culprit, but in the end, the SODA API was responsible.‚Ü©Ô∏é\nThe process will get frozen at some point with no notifications or feedback.‚Ü©Ô∏é\nLittle Mai and I üêë.‚Ü©Ô∏é\nThis occurs when the returned object is a data frame (Polars or Pandas). In this concrete case, that won‚Äôt happen since we‚Äôre returning a dictionary, but in the following blocks, it will be the case since the objects transferred will be Polars data frames. With dictionaries, serialization will still occur but in JSON format.‚Ü©Ô∏é\nWe can also use .pl() to cast the results into a Polars data frame or .df() for a Pandas one.‚Ü©Ô∏é\nFor instance, I should move the fetch_batch function outside the load_data_from_api loader. Defining functions within other functions is generally considered a bad practice, as it can make the code harder to read, test, and maintain.‚Ü©Ô∏é\nRepeat after me: ‚ÄúWe love open source.‚Äù‚Ü©Ô∏é\nWe‚Äôve found Manning a reliable editorial for technical books. Most of the time, their materials have high-quality content that is easy to digest. Highly recommended üëå.‚Ü©Ô∏é\nThe only action we did to enable the use of DuckDB was pip install duckdb.‚Ü©Ô∏é\nPlease note that this is not a traditional exporter block; it could be a custom block. I chose the exporter type because it interacts with the local database and creates the table, even though no data is exported in an explicit manner.‚Ü©Ô∏é\nFree Bird‚Äôs guitar solo starts playing in the background üé∏üé∂üé∂.‚Ü©Ô∏é\nIf you‚Äôve had the pleasure to work with geospatial analysis before, you probably already know that CRSs are always awfully documented (i.e., not documentation at all). So, working with coordinates of less standard regions is a painful process full of guesses.‚Ü©Ô∏é\nIt is more to illustrate that DuckDB can be the source that feeds different systems. We already checked R, and we‚Äôre continuing with Python. You can follow with Power BI or Tableau if you want :)‚Ü©Ô∏é"
  },
  {
    "objectID": "data_pipeline_mage.html#soda-api-our-data-source",
    "href": "data_pipeline_mage.html#soda-api-our-data-source",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "SODA API: our data source",
    "text": "SODA API: our data source\nThe Iowa Liquor Sales data is provided by the Iowa government through the Socrata Open Data API (SODA), a platform designed to grant access to open datasets from government agencies, non-profits, and other organizations. This API allows us to programmatically interact with our dataset of interest, enabling us to retrieve data via HTTP requests.\nThe dataset‚Äôs documentation provides the key information needed to access the data: the source domain (data.iowa.gov) and the dataset identifier (m3tr-qhgy). With this information, we can define the endpoint for sending our requests. The documentation also details the available variables and their respective data types. Below is an example of a base endpoint for this dataset:\n\nhttps://data.iowa.gov/resource/m3tr-qhgy.csv\n\nThe dataset documentation also informs us that, to date, it consists of more than 30 million rows and 24 variables. Now that we know the endpoint and the size of the data, we‚Äôre ready to pull the data, right?\nA sensible approach would be to retrieve the data sequentially using the paging method described in the SODA documentation. This method allows us to fetch the data in manageable batches, specified by the $limit parameter, while navigating through the dataset using the $offset parameter. The process is illustrated in the following diagram:\n\n\n\nLater rows of the data can‚Äôt be fetched due to system limits.\n\n\nNaturally, I started with this approach to find a limitation in the API that was not documented. When we get to the point of pulling the records around row 20M, the data loader will get stuck, and no information will be pulled. This might be by design or due to system limitations in which deep paginations can bog the system4, making this an unreliable method to get the data5. Meaning that we will need a different strategy to retrieve the 30M records.\nThis brings us to the next strategy: using SoQL, the query language of the Socrata API. SoQL is quite similar to SQL, with the key difference that its syntax is structured to work within a URL format.\nWe will send an HTTP request to the server to query individual batches of invoices corresponding to a specific year (based on the date variable). We represent this as follows.\n\n\n\nFetching each year individually: an incremental approach. The queries in the blocks are written in SQL for readability.\n\n\nThis approach limits pagination to the number of years in the dataset rather than the total number of records. By fetching data in yearly batches, our requests won‚Äôt get stuck at an offset of 20 million, as each year contains fewer than 3 million records.\nWhy are we spending so much time on this? Understanding our data source‚Äôs quirks, perks, and limitations is crucial because, ultimately, this knowledge will shape how we design our data pipeline. So it‚Äôs worth it to spend some time understanding the source system; it will save us headaches and result in a better design that is easier to maintain from the beginning.\nNow, we have a clearer understanding of the upper stage in our stream, the data source.\n\n\n\nProgress we‚Äôve made so far.\n\n\nWith this in mind, we can now move on to the ingestion step."
  },
  {
    "objectID": "data_pipeline_mage.html#fetching-the-data",
    "href": "data_pipeline_mage.html#fetching-the-data",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Fetching the data",
    "text": "Fetching the data\nWe are aware of the endpoint to request the data and some limitations of this API. We know that we cannot fetch the data as it is because, at some point, our pipeline will clog, and we won‚Äôt be able to request more data from the SODA DB.\nDue to this, we will write more complex HTTP requests using SoQL, the SODA query language. SoQL syntax is similar to SQL, so if you are familiar with relational databases, learning will feel intuitive.\nOur base query to fetch the data will look something like this:\nhttps://data.iowa.gov/resource/m3tr-qhgy.csv?$where=date_extract_y(date)=2013&$limit=2000\n\nTeal: the endpoint for the data request.\nBlue: the WHERE clause of the SoQL query, used to filter records for the year 2013. The function date_extract_y() extracts the year from the date column.\nOrange: the LIMIT clause of the SoQL query, used to limit the response to 2000 records. If we don‚Äôt indicate this, the default will be used, which is 1000 records.\n\nNotice that the ? symbol separates the endpoint from the query parameters, and each clause is separated by an & symbol. To indicate the start of a clause, a $ is used.\nThis request will be sent using the GET HTTP method.\nWith this in mind, we can return to Mage. We‚Äôll begin by creating the first three blocks, which will focus on retrieving metadata. The primary purpose of these blocks is to provide the downstream block with the necessary information on what data to fetch from the endpoint and how to fetch it.\n\n\n\n\n\n\nNote\n\n\n\nIn this tutorial, we will adopt the writing style of The Rust Programming Language book, where the file path is shown before each code block. This approach allows you to easily follow the project‚Äôs structure. The complete code for the project is available on the GitHub repo. Also, the function docstrings have been removed from this article. However, you can review the complete technical documentation of each function by checking the files in the repo.\n\n\n\nWriting our first Mage blocks\nHere we will implement our first blocks, two of them will fetch metadata from the SODA API, and another one will get metadata from our local database (DuckDB). First lets go to our already created pipeline, socrata_iowa_liquor_pipeline. If you want to follow this tutorial from scratch, you can create a fresh pipeline (for instance, socrata_iowa_liquor_pipeline_from_scratch) and continue from there. With the intuitive Mage UI, it will be easy for you to create this new pipeline.\nOur first block:\n\n\n\nCreating our first block.\n\n\nHere, select the first option, Data loader, then navigate to the creation option and select Python as the language, and then API as the source, then name the block get_schema_from_metadata. When created, the block will come with a template like this:\nFilename: data_loaders/get_schema_from_metadata.py\n\n1import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n2@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = ''\n    response = requests.get(url)\n\n    return pd.read_csv(io.StringIO(response.text), sep=',')\n\n\n3@test\ndef test_output(output, *args) -&gt; None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n\n\n1\n\nImports: block‚Äôs prelude.\n\n2\n\nMain functionality: since this is a data loader it has the @data_loader decorator on top.\n\n3\n\nTest for data validation: accepts the output data of the block. If any of the tests fail, the block execution will also fail.\n\n\n\n\nMage provides a variety of templates designed to guide you and save time. These templates were incredibly helpful when we6 first started building data pipelines. Notices the structure of a block, it is composed by three sections: library imports, a function, and a test. The function handles the main task of the block, while the test ensures that the block‚Äôs main functionality works as intended.\nNext, we‚Äôll modify the template to suit our specific use case. Overwrite the content of your newly created block with the following code, and let‚Äôs proceed to analyze it:\nFilename: data_loaders/get_schema_from_metadata.py\n\nimport io\n1import polars as pl\n2import requests\nif 'data_loader' not in globals():\n3    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n4    from mage_ai.data_preparation.decorators import test\n\n# Map API data types to Polars types\n5SODA_TO_POLARS = {\n    \"text\": pl.Utf8,\n    \"number\": pl.Float64, \n    \"calendar_date\": pl.Datetime(\"us\"),\n    \"floating_timestamp\": pl.Datetime(\"us\"),  \n}\n\n\n# Loads the schema (i.e., types) of our data set\n6@data_loader\ndef load_data_schema_from_api(*args, **kwargs):\n\n7    url = 'https://{DOMAIN}/api/views/{DATASET_ID}'.format(**kwargs)\n    response = requests.get(url)\n\n    metadata = response.json()\n    columns = metadata.get(\"columns\", [])\n    schema = {\n8        col[\"fieldName\"]: SODA_TO_POLARS.get(col[\"dataTypeName\"], pl.Utf8)\n        for col in columns\n9        if not col[\"fieldName\"].startswith(\":@computed_\")\n        }\n    \n    return schema\n\n10@test\ndef test_output(dictionary, *args) -&gt; None:\n    assert dictionary is not None, \"The output is undefined\"\n    assert isinstance(dictionary, dict), \"The output is not a dictionary\"\n    assert len(dictionary) &gt; 0, \"The dictionary is empty\"\n\n\n1\n\nPolars: used for data manipulation and type mapping throughout the project.\n\n2\n\nHTTP library: requests is used to send HTTP requests to the API.\n\n3\n\nLoader decorator: the @data_loader decorator marks the function as a Mage data loader block.\n\n4\n\nTest decorator: the @test decorator defines a test function for validating the output.\n\n5\n\nSocrata to Polars type mapping: the SODA_TO_POLARS dictionary maps Socrata data types to corresponding Polars types, ensuring compatibility.\n\n6\n\nFunction decoration: the loader function load_data_schema_from_api is decorated with @data_loader to integrate it into the Mage pipeline. It is important to include the decorator to power the function we define with Mage functionalities.\n\n7\n\nEndpoint definition: the endpoint URL is dynamically generated using the global variables DOMAIN and DATASET_ID. This is where the schema metadata is fetched.\n\n8\n\nDictionary comprehension: the schema dictionary maps column names to their corresponding Polars types, based on the SODA_TO_POLARS dictionary. Unrecognized types default to Utf8.\n\n9\n\nExclusion columns: columns with names starting with :@computed_ are excluded from the schema.\n\n10\n\nTest function: the test validates the loader‚Äôs output, ensuring it is a non-empty dictionary of the correct type. Remember to include the @test decorator so the test works as Mage intended.\n\n\n\n\nThe primary goal of the get_schema_from_metadata block is to retrieve data type information to standardize batch ingestion. While Polars can infer data types based on the content it reads, each batch may differ in structure. This inconsistency can lead to errors and prevent us from merging batches later. We can resolve this issue by specifying data types and ensuring a consistent schema across all batches.\nPlease notice that we use Polars for data transformation and manipulation. Compared with Pandas, Polars offers better performance and scalability, making it a more cost-effective choice for production systems. This case study demonstrates that Polars can reduce computational costs, making it a good choice for projects that may eventually transition to production. Polars is a mature and robust library with extensive capabilities, making it an excellent choice for this project.\nWe can illustrate how the Mage flow operates using our first block. Initially, the block performs its main task‚Äîfetching data from an API in this example. The instructions for this task are defined in the block‚Äôs primary function, which is identified by the @data_loader decorator. Once the main task is completed, its output is passed to the tests, marked with the @test decorator. If all tests are successful, the output is forwarded to the next block in the stream.\nWhen you first looked at Named figure¬†1, you might have wondered why storage spans the entire process and why Apache Arrow and Parquet are included in the storage block alongside DuckDB. This is because data storage underpins every major stage, with data being stored multiple times throughout its life cycle. Mage uses PyArrow to handle data serialization and deserialization between blocks during these storage steps7. For disk storage, PyArrow serializes the data into the Parquet format. For more details, you can check Mage‚Äôs documentation.\nBelow is an overview of how Mage blocks function:\n\n\n\n\n\n\nNamed figure¬†2: Overview of Mage blocks functioning.\n\n\n\nCongrats! You just implemented your first Mage block. Let‚Äôs test it, you can do this with the ‚Äúplay‚Äù icon in the header of the block:\n\nAfter the block runs, you should be able to see the output in the tail of the block:\n\nWe see a message indicating that the test passed and how Mage tries to display the output dictionary.\nNext, we will query the number of records (invoices) per year. To achieve this, we will create a data loader block named soda_records_per_year. The content of this block will look as follows:\nFilename: data_loaders/soda_records_per_year.py\n\n1import io\nimport polars as pl\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n2@data_loader\ndef load_data_from_api(*args, **kwargs):\n    # SoQL to get the the number of invoices per year\n    data_url = \"https://data.iowa.gov/resource/m3tr-qhgy.csv?$select=date_extract_y(date) AS year, count(invoice_line_no) AS rows&$group=date_extract_y(date)\"\n    print(\"\\n\")\n    print(\"Fetching the records-per-year metadata. This might take a couple minutes..\")\n    response = requests.get(data_url)\n    print(\"Done! We have our year record metadata.\\n\")\n    data = pl.read_csv(io.StringIO(response.text))\n    \n    return data\n\n\n3@test\ndef test_output(output, *args) -&gt; None:\n    assert output is not None, 'The output is undefined'\n    assert isinstance(output, pl.DataFrame), 'The output is not a Polars DataFrame'\n\n\n1\n\nYou might have noticed that we use the io module to handle the API responses. Specifically, we utilize the StringIO() class to treat the response as a document. Otherwise, Polars will complain.\n\n2\n\nHere, we make a SoQL query to the API, and load the response into a Polars data frame.\n\n3\n\nWe also validate that the block‚Äôs output is not empty and confirm that it is a Polars data frame.\n\n\n\n\nIn this block we request the records per year to the API, for this we make use of the SoQL language provided by the API developers:\nhttps://data.iowa.gov/resource/m3tr-qhgy.csv?$select=date_extract_y(date) AS year, count(invoice_line_no) AS rows&$group=date_extract_y(date)\nAnd again, we can dissect this request:\n\nTeal: endpoint.\nBlue: query.\n\nIf we translate this query to SQL, we would have something like this:\n\nSELECT YEAR(date) AS year\n       COUNT(invoice_line_no) AS rows\nFROM m3tr-qhgy -- Iowa Liquor Sales table\nGROUP BY YEAR(date)\n\nThe output should be a table displaying the number of records for each year. This information will be used to determine the $limit clause when retrieving data later.\nTo complete the upstream blocks, we will create another block to check the data available in our local DuckDB database. I named this block check_local_db. It is of type ‚Äúcustom,‚Äù though it could also have been implemented as a data loader.\nFilename: custom/check_local_db.py\n\n# -- snip --\n\n1db_path = 'data/iowa_liquor.duckdb'\n\n@custom\ndef check_last_year(*args, **kwargs):\n2    conn = duckdb.connect(\"data/iowa_liquor.duckdb\")\n    \n3    try:\n        result = conn.execute(\"\"\"\n        SELECT EXTRACT(YEAR FROM MAX(date)) FROM iowa_liquor_sales\n        \"\"\").fetchall()\n        last_year = result[0][0]\n\n4    except duckdb.CatalogException as e:\n        print(\"The table doesn't exist; assigning rows_in_db=0\")\n        last_year = 0\n\n5    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        last_year = 0\n\n    # Close DB connections\n6    conn.close()\n\n7    return last_year\n\n# -- snip --\n\n\n1\n\nDefine the path to the database. For this project, the database will be stored in the file data/iowa_liquor.duckdb.\n\n2\n\nEstablish a connection to the local database. If the file iowa_liquor.duckdb does not exist, DuckDB will automatically create it.\n\n3\n\nQuery the date of the most recent invoice in the database and extract the year. Use .fetchall() to retrieve the query output as a Python object8.\n\n4\n\nIf the table does not yet exist in the database, return 0 as the year.\n\n5\n\nIf any other exception occurs, also return 0 as the year.\n\n6\n\nExplicitly close the database connection to avoid blocking access for downstream processes.\n\n7\n\nReturn the most recent year as an int.\n\n\n\n\nThis block is designed to monitor the local database, enabling efficient updates by avoiding the need to fetch all data every time. This approach ensures faster processing and reduces resource consumption.\nWith that, we‚Äôve completed the first layer of Mage blocks. The next step downstream involves retrieving the data of interest. In this stage, we will use the outputs of the three blocks we just created\n\n\nPulling the Iowa Liquor Sales data\nWith the metadata in place, we can now retrieve the sales data of interest by creating the pull_data_from_socrata block. Once this loader block is created, connect the upstream blocks to it in the following order:\n\nget_schema_from_metadata\nsoda_records_per_year\ncheck_local_db\n\nYou can make this configuration in the ‚ÄúTree‚Äù panel to the right of your Mage code editor:\n\nThe final configuration should look like this:\n\nThis will ensure the parameter order remains consistent for you as well. While we know this block will utilize data from the three upstream blocks, how do we access this data in our code? The answer lies in the header of our new block, which contains all the information we need:\n\nIn this header, we receive all the necessary information to consume the upstream data. It includes an example showing the parameter order and how they correspond to each upstream block. The loader function, identified by the @data_loader decorator, provides a clear structure.\nThe parameter names (data, data_2, data_3) are arbitrary, allowing you to choose names that best suit your function. However, what matters most is the positional order of the parameters. Specifically:\n\nThe first parameter corresponds to the output of get_schema_from_metadata.\nThe second parameter corresponds to soda_records_per_year.\nThe third parameter corresponds to check_local_db.\n\nWith this in mind, we can proceed to define the content of this block. Since there is a lot to cover, we‚Äôll break it down into three sections: the imports, the loader function, and the tests. Let‚Äôs start with the dependencies for this block:\nFilename: data_loaders/pull_data_from_socrata.py\n\nimport io\nimport polars as pl\nimport requests\n1from math import ceil\n2from concurrent.futures import ThreadPoolExecutor, as_completed\n3from tqdm import tqdm\n\n# -- snip --\n\n\n1\n\nRound numbers to the nearest ceiling.\n\n2\n\nWe will implement a multithreading approach to speed up the data retrieval process.\n\n3\n\nA progress bar. Provides real-time feedback on the data-pulling progress, which besides of being helpful for the user, it‚Äôs also supportive for debugging purposes.\n\n\n\n\nNothing weird in the imports. Now, let‚Äôs review the most complex part of our implementation, the loader function:\nFilename: data_loaders/pull_data_from_socrata.py\n\n# -- snip --\n\n@data_loader\n1def load_data_from_api(schema,\n                       records_per_year,\n                       last_year_in_local_db,\n                       *args, **kwargs):\n\n2    DOMAIN     = 'data.iowa.gov'\n    DATASET_ID = 'm3tr-qhgy'\n    base_url   = f\"https://{DOMAIN}/resource/{DATASET_ID}.csv?\"\n    query_url  = \"$where=date_extract_y(date)={}&$limit={}\"\n    records_per_year = records_per_year.with_columns(\n    pl.format(base_url + query_url, pl.col(\"year\"), pl.col(\"rows\")).alias(\"url\")\n    )\n\n3    records_per_year = records_per_year.filter(pl.col(\"year\") &gt;= last_year_in_local_db)\n\n    if (last_year_in_local_db != records_per_year[\"year\"].max()) & last_year_in_local_db != 0:\n        # We're limiting to five years per job so our machine dont explode\n        records_per_year = records_per_year.sort(\"year\").head(6)\n        records_per_year = records_per_year.filter(pl.col(\"year\") != last_year_in_local_db)\n    \n    # We're limiting to five years per job so our machine dont explode\n4    records_per_year = records_per_year.sort(\"year\").head(5)\n\n    # Years we will request to the API\n5    requests_list = records_per_year[\"url\"].to_list()\n\n    print(\"SODA data pull started.\")\n\n    # Report to the user which years we will be working with\n    years_to_fetch = records_per_year[\"year\"].to_list()\n    print(\"Years to be fetched: {}.\".format(\", \".join(map(str, years_to_fetch))))\n    \n# -- snip --\n\n\n1\n\nParameter order: remember, the order of the parameters matters, but you can name them however you like.\n\n2\n\nCustomizing API requests: The records_per_year data frame is used to construct the URLs for API requests. It customizes the SoQL query for each year, with a unique $limit value tailored to the data volume for that year.\n\n3\n\nExcluding existing data: an additional filter ensures that years already present in the local database are excluded from the requests.\n\n4\n\nData pull limit: to prevent overloading your machine, we limit data pulls to a maximum of five years per run. Adjust this limit based on your system‚Äôs capacity.\n\n5\n\nURL list: after filtering, the formatted URLs are collected into a list, which will be used later for parallel data fetching.\n\n\n\n\nUp to this point, the process has been straightforward. We simply used some upstream data (records_per_year) to format the URLs needed for API requests, applying a few Polars transformations for text formatting. Lets continue reviewing the concurrent fetch of the data.\nFilename: data_loaders/pull_data_from_socrata.py\n\n# -- snip --\n\n@data_loader\ndef load_data_from_api(schema,\n                       records_per_year,\n                       last_year_in_local_db,\n                       *args, **kwargs):\n\n# -- snip --\n\n1    def fetch_batch(data_url):\n        \"\"\"Fetch data for a given URL.\"\"\"\n        try:\n            response = requests.get(data_url)\n            response.raise_for_status()  # Raise an error for bad responses\n            return pl.read_csv(io.StringIO(response.text), schema = schema)\n        except Exception as e:\n            print(f\"Error fetching data from {data_url}: {e}\")\n            return pl.DataFrame(schema = schema)\n\n    # Use ThreadPoolExecutor for concurrent API calls\n    df_list = []\n2    with ThreadPoolExecutor(max_workers = 3) as executor:\n        \n3        futures = {executor.submit(fetch_batch, url): url for url in requests_list}\n4        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Fetching data\"):\n            url = futures[future]\n            try:\n5                data = future.result()\n                if not data.is_empty():\n                    df_list.append(data)\n            except Exception as e:\n                print(f\"Error processing URL {url}: {e}\")\n\n6    all_pulled_data = pl.concat(df_list, how=\"vertical\")\n\n7    return all_pulled_data\n\n\n1\n\nFirst we have to define a function that will handle the requests. It receives an URL, makes the GET request, and returns a Polars data frame with the data. In case there is an error in our request, it also handles that.\n\n2\n\nInitializes a thread pool with a maximum of three workers (max_workers = 3), allowing up to three API requests to be executed concurrently. This will help to speed up the data retrieval. The with statement is used for context management, ensuring the thread pool is properly shut down once all threads have completed their work.\n\n3\n\nHere, we submit tasks to the thread pool. A dictionary is created using comprehension. The executor.submit(...) method schedules the fetch_batch function to run in a separate thread with url as its argument, returning a Future object. The resulting futures dictionary maps these Future objects to their corresponding URLs.\n\n4\n\nTo monitor progress, the code iterates over the Future objects as they complete (using as_completed) and tracks progress with tqdm, providing real-time feedback to the user.\n\n5\n\nRetrieves the results, for each completed task, future.result() is called to retrieve the result of the fetch_batch function. This method waits for the task to finish and returns the resulting Polars data frame.\n\n6\n\nAll data frames are merged into a single one.\n\n7\n\nFinally, the merged data frame containing all the requested data is returned.\n\n\n\n\nSo, this function utilizes ThreadPoolExecutor to submit API requests for all the URLs in requests_list. Tasks are executed in parallel based on the number of workers specified. Progress is monitored using tqdm, which updates the progress bar in real-time. The result (a Polars data frame) is retrieved and validated for each completed task. Once all tasks are complete, the individual data frames are merged into a single one, which is then returned. The process is illustrated in the following chart.\n\n\n\nAsynchronous data fetching\n\n\n\n\nTesting\nWith the main functionality complete, ensuring that the loader runs reliably is essential, and equally important, to run automatic tests after making adjustments to the code9. To achieve this, we write tests to verify its behavior. Additionally, to maintain the best data quality in our pipeline, these tests also focus on ensuring the integrity and consistency of the data.\nLet‚Äôs review the final part of our block, which consists of three tests. These tests are designed to validate certain expectations of the data, such as ensuring no null values in specific columns, and to verify the overall integrity of the data.\nFilename: data_loaders/pull_data_from_socrata.py\n\n# -- snip --\n\n1@test\ndef test_output(output, *args) -&gt; None:\n    \"\"\"\n    Validates the output of data pulling block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert isinstance(output, pl.DataFrame), \"The output is not a Polars data frame\"\n    assert len(output) &gt; 0, \"The data frame is empty\"\n\n2@test\ndef test_invoice_line_no_not_null_output(output, *args) -&gt; None:\n    \"\"\"\n    Test the new invoice_line_no column contains no nulls.\n    \"\"\"\n    assert output[\"invoice_line_no\"].is_null().sum() == 0, \"The invoice_line_no column contains null values, it shouldn't\"\n\n3@test\ndef test_date_not_null_output(output, *args) -&gt; None:\n    \"\"\"\n    Test the new date column contains no nulls.\n    \"\"\"\n    assert output[\"date\"].is_null().sum() == 0, \"The date column contain null values, it shouldn't\"\n\n\n1\n\nVerify that the output is a non-empty Polars data frame containing valid content.\n\n2\n\nEnsure that the invoice_line_no column contains no null values.\n\n3\n\nConfirm that the date column contains no null values.\n\n\n\n\nWhen writing tests for your blocks, always include the @test decorator. This informs Mage to treat the functions as tests. In these examples, we verified the basic integrity of the resulting data frame and enforced specific rules for two columns. If everything went smoothly, you should see a confirmation message for each test that passed after running the block.\n\n\n\nProgress we‚Äôve made so far.\n\n\nAnd we just finished implementing the data ingestion part of our pipeline! :D"
  },
  {
    "objectID": "data_pipeline_mage.html#transform-the-data",
    "href": "data_pipeline_mage.html#transform-the-data",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Transform the data",
    "text": "Transform the data\nWith the retrieved data in hand, we now focus on transforming it. As illustrated in Named figure¬†2, after fetching the data and running the tests, the block serializes the data to pass it downstream. The next step is to implement these downstream blocks to transform the data and generate new variables.\n\nHave present the bussiness logic\nThe dataset offers a wealth of information, such as product descriptions and geolocation points, providing ample opportunities for deeper insights and analysis. To enhance this further, we will create new variables that reveal additional business insights, enabling us to answer more interesting questions. We‚Äôve organized the transformations into the following business-logic blocks:\n\nproduct_transformations: generates general product descriptions, such as liquor type or packaging size.\nsales_and_price_related_metrics: creates variables like profits and costs.\nvolume_based_features: develops features related to the liquid volume (Liters) of sales.\ntime_based_features: generates temporal variables for time-based analysis.\n\n\n\nLet‚Äôs transform our data\nMost of the code in these blocks will be Polars transformations. If you are familiar with PySpark you will find that the syntax is similar, no worries if this is your first time with Polars, let‚Äôs review what the code does.\nFilename: transformers/product_transformations.py\n\n# -- snip --\n\n@transformer\ndef transform(data, *args, **kwargs):\n    data = data.with_columns(\n        # Categorize liquors\n1        pl.when(pl.col(\"category_name\").str.contains(\"VODK\")).then(\"Vodka\")\n        .when(pl.col(\"category_name\").str.contains(\"WHISK\")).then(\"Whisky\")\n        .when(pl.col(\"category_name\").str.contains(\"RUM\")).then(\"Rum\")\n        .when(pl.col(\"category_name\").str.contains(\"SCHN\")).then(\"Schnapps\")\n        .when(pl.col(\"category_name\").str.contains(\"TEQ\")).then(\"Tequila\")\n        .when(\n            pl.col(\"category_name\").str.contains(\"BRANDIE\")\n            | pl.col(\"category_name\").str.contains(\"BRANDY\")\n            ).then(\"Brandy\")\n        .when(pl.col(\"category_name\").str.contains(\"GIN\")).then(\"Gin\")\n        .when(pl.col(\"category_name\").str.contains(\"MEZC\")).then(\"Mezcal\")\n        .when(\n            pl.col(\"category_name\").str.contains(\"CREM\")\n            | pl.col(\"category_name\").str.contains(\"CREAM\")\n            ).then(\"Cream\")\n        .otherwise(\"Other\")\n        .alias(\"liquor_type\"),\n        # Is premium\n2        (pl.col(\"state_bottle_retail\") &gt;= 30).alias(\"is_premium\"),\n        # Bottle size category\n3        pl.when(pl.col(\"bottle_volume_ml\") &lt; 500).then(\"small\")\n        .when((pl.col(\"bottle_volume_ml\") &gt;= 500) & (pl.col(\"bottle_volume_ml\") &lt; 1000)).then(\"medium\")\n        .otherwise(\"large\")\n        .alias(\"bottle_size\")\n        )\n\n    print(\"Product-related new variables, generated.\")\n    return data\n\n# -- snip --\n\n\n1\n\nCategorize the liquor type (Tequila, Whisky, etc.).\n\n2\n\nDetermine whether the product is considered premium based on its individual price.\n\n3\n\nClassify the product by size.\n\n\n\n\nIn this context, data represents the upstream DataFrame passed to this block by pull_data_from_socrata. It is the first parameter of the transform transformer. The Polars transformation begins with the with_columns method, which is used to add or replace columns in the DataFrame.\nWithin with_columns, we define the transformations. To create the first new variable, we use the pl.when() function to start a conditional statement. To reference a column, we use the pl.col() function. If you are only familiar with Pandas or dplyr, this might feel counterintuitive at first, but it works in a similar fashion‚Äîjust pass the column name as a string, e.g., pl.col(\"your_col\").\nOnce the column is selected, we apply the .str.contains() method to check if the string in a cell contains a specified pattern. If the pattern matches, the value specified in .then() will be used. We can chain additional .when() statements for more conditions, and the default value, used when no condition is met, is specified with .otherwise(\"Other\"). To name the new column, we use .alias(\"liquor_type\"). If the name matches an existing column, it will overwrite that column.\nThis transformation leverages chained operations, a powerful feature in Polars. Most Polars methods return a DataFrame, allowing you to immediately chain additional methods. This enhances code readability and makes debugging easier. Note that after the .alias() method, there is a comma , instead of a dot ., indicating the end of the chain.\nNext, we proceed with premium and size categorizations in a similar manner.\n\n\nTest, test, test!\nOnce the new variables are created, we will write tests for the outputs to ensure a baseline level of quality for the new data.\nFilename: transformers/product_transformations.py\n\n# -- snip --\n\n1@test\ndef test_output(output, *args) -&gt; None:\n    assert output is not None, 'The output is undefined'\n\n2@test\ndef test_liquor_type_col(output, *args) -&gt; None:\n    assert output.get_column(\"liquor_type\") is not None, 'The column liquor_type is undefined'\n    assert output.get_column(\"liquor_type\").dtype is pl.Utf8, \"The new variable type doesn't match\"\n\n3@test\ndef test_is_premium_col(output, *args) -&gt; None:\n    assert output.get_column(\"is_premium\") is not None, 'The column is_premium is undefined'\n    assert output.get_column(\"is_premium\").dtype is pl.Boolean, \"The new variable type doesn't match\"\n\n4@test\ndef test_bottle_size_col(output, *args) -&gt; None:\n    assert output.get_column(\"bottle_size\") is not None, 'The column bottle_size is undefined'\n    assert output.get_column(\"bottle_size\").dtype is pl.Utf8, \"The new variable type doesn't match\"\n\n\n1\n\nValidate the overall output has content.\n\n2\n\nVerify the liquor_type column‚Äôs content and data type.\n\n3\n\nCheck the is_premium column‚Äôs content and data type.\n\n4\n\nEnsure the bottle_size column‚Äôs content and data type are correct.\n\n\n\n\nNotice that each new variable has at least one associated test. While more detailed tests can be added later, these initial ones will suffice for now. And that‚Äôs essentially how a transformation block is structured. The next three transformation blocks follow a similar pattern, so I won‚Äôt explain them in detail, but feel free to review them as needed.\n\nsales_and_price_related_metrics\nvolume_based_features\ntime_based_features\n\nThat concludes our data transformation step! Next, we‚Äôll move on to storing our valuable data.\n\n\n\nProgress we‚Äôve made so far."
  },
  {
    "objectID": "data_pipeline_mage.html",
    "href": "data_pipeline_mage.html",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "",
    "text": "WARNING: Please take into account that some expressions won‚Äôt make sense, the doc is unorganized, and plagued of typos. I published this early since it is easier for me to edit the repo and the article this way :)"
  },
  {
    "objectID": "data_pipeline_mage.html#storage",
    "href": "data_pipeline_mage.html#storage",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Storage",
    "text": "Storage\nWith a robust method for retrieving and transforming our data, the next step is to store and query it efficiently. For this purpose, we‚Äôll use DuckDB. But why choose DuckDB?\nDuckDB is well-known for its in-memory data manipulation capabilities, but it also supports persistent storage with several compelling advantages. By using DuckDB‚Äôs native storage format, we gain the following benefits:\n\nFree and open source: DuckDB is free and open-source, aligning with our preference for accessible and transparent tools10.\nLightweight compression: it employs advanced compression techniques to reduce storage space while maintaining efficiency.\nColumnar vectorized query execution: DuckDB processes data in chunks (vectors) rather than row-by-row, making efficient use of CPU and memory. This design is optimized for modern hardware architectures. For a comprehensive explanation, see Kersten et al.¬†(2018).\nCompact file structure: DuckDB stores data in a compact, self-contained binary file, making it portable and easy to manage.\nComprehensible resources: besides excellent documentation, DuckDB has a fantastic community that makes it easy to use this database system. You can get a free ‚ÄúDuckDB in Action‚Äù copy on the MotherDuck webpage11.\n\nDue to this and its ease of use12, DuckDB is an ideal tool for our purpose. So let‚Äôs continue with the implementation of our storage step.\nAfter we have finished with our data transformers, we can move to the data exporters. Here, we could have done this by creating only one data exporter, but for the sake of keep the blocks simple and modular (only one task per block) I decided to create two blocks for the data exportation, one for creating the table we will use to store our data, and a last one to export the data. We will start by analyzing the block responsible of the table creation.\nFilename: data_exporters/create_duckdb_table.py\n\nimport os\nimport duckdb\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n# -- snip --\n\nWe specify the query for table creation:\nFilename: data_exporters/create_duckdb_table.py\n\n# -- snip --\n\ncreate_table_query = \"\"\"\n    CREATE TABLE IF NOT EXISTS iowa_liquor_sales (\n        invoice_line_no TEXT PRIMARY KEY NOT NULL,\n        date TIMESTAMP NOT NULL,\n        store TEXT,\n        name TEXT,\n        address TEXT,\n        city TEXT,\n        zipcode TEXT,\n        store_location TEXT,\n        county_number TEXT,\n        county TEXT,\n        category TEXT,\n        category_name TEXT,\n        vendor_no TEXT,\n        vendor_name TEXT,\n        itemno TEXT,\n        im_desc TEXT,\n        pack REAL,\n        bottle_volume_ml REAL,\n        state_bottle_cost REAL,\n        state_bottle_retail REAL,\n        sale_bottles REAL,\n        sale_dollars REAL,\n        sale_liters REAL,\n        sale_gallons REAL,\n        liquor_type TEXT,\n        is_premium BOOLEAN,\n        bottle_size TEXT,\n        gov_profit_margin REAL,\n        gov_retail_markup_percentage REAL,\n        price_per_liter REAL,\n        price_per_gallon REAL,\n        total_volume_ordered_L REAL,\n        volume_to_revenue_ratio REAL,\n        week_day INTEGER,\n        is_weekend BOOLEAN,\n        quarter INTEGER\n);\n\"\"\"\n\n# -- snip --\n\nExporter:\nFilename: data_exporters/create_duckdb_table.py\n\n# -- snip --\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    # Create a connection to a file called 'data/iowa_liquor.duckdb'\n    # If the file doesn't exist it should create it\n    conn = duckdb.connect(\"data/iowa_liquor.duckdb\")\n    # Create a table and load data into it\n    conn.sql(create_table_query)\n    # Explicitly close the connection\n    conn.close()\n    \n    # Pass the unmodified data\n    return data\n\n# -- snip --\n\nTests: Filename: data_exporters/create_duckdb_table.py\n\n# -- snip --\n\n@test\ndef test_output(output, *args) -&gt; None:\n    \"\"\"\n    Test the output exists.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n\n@test\ndef db_exist(*args) -&gt; None:\n    \"\"\"\n    Test there is a .duckdb file.\n    \"\"\"\n    assert os.path.exists(\"data/iowa_liquor.duckdb\"), \"The database file doesnt exist\"\n\nBash execution\n\nuser@user-pc:~/mage_duckdb_pipeline$ mage run . socrata_iowa_liquor_pipeline\nFetching the records-per-year metadata. This might take a couple minutes..\nDone! We have our year record metadata.\n\nSODA data pull started.\nYears to be fetched: 2017, 2018, 2019, 2020, 2021.\nFetching data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [10:08&lt;00:00, 121.69s/it]\nProduct-related new variables, generated.\nSales and price related metrics, computed.\nVolume-based features, computed.\nTime-based features, computed.\nData loaded to your DuckDB database!\nPipeline run completed.\n\n\nconn2 &lt;- DBI::dbConnect(\n  drv = duckdb::duckdb(),\n  dbdir = \"../data/iowa_liquor.duckdb\",\n  read_only = TRUE\n)\n\n\nimport duckdb\n\ncon_py = duckdb.connect(\"../data/iowa_liquor.duckdb\", read_only=True)\n\npolars_df = con_py.sql(\"SELECT liquor_type, bottle_size, price_per_liter FROM iowa_liquor_sales LIMIT 10\").pl()\n\npolars_df\n\n\npolars_df.height"
  },
  {
    "objectID": "data_pipeline_mage.html#persistent-storage-and-serving",
    "href": "data_pipeline_mage.html#persistent-storage-and-serving",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Persistent storage and serving",
    "text": "Persistent storage and serving\nWith a robust method for retrieving and transforming our data, the next step is to store and query it efficiently. For this purpose, we‚Äôll use DuckDB. But why choose DuckDB?\nDuckDB is well-known for its in-memory data manipulation capabilities, but it also supports persistent storage with several compelling advantages. By using DuckDB‚Äôs native storage format, we gain the following benefits:\n\nFree and open source: DuckDB is free and open-source, aligning with our preference for accessible and transparent tools10.\nLightweight compression: it employs advanced compression techniques to reduce storage space while maintaining efficiency.\nColumnar vectorized query execution: DuckDB processes data in chunks (vectors) rather than row-by-row, making efficient use of CPU and memory. This design is optimized for modern hardware architectures. For a comprehensive explanation, see Kersten et al.¬†(2018).\nCompact file structure: DuckDB stores data in a compact, self-contained binary file, making it portable and easy to manage.\nComprehensible resources: besides excellent documentation, DuckDB has a fantastic community that makes it easy to use this database system. You can get a free ‚ÄúDuckDB in Action‚Äù copy on the MotherDuck webpage11.\n\nDue to this and its ease of use12, DuckDB is an ideal tool for our purpose. So let‚Äôs continue with the implementation of our storage step.\nAfter we have finished with our data transformers, we can move to the data exporters. Here, we could have done this by creating only one data exporter, but for the sake of keep the blocks simple and modular (only one task per block) I decided to create two blocks for the data exportation, one for creating the table we will use to store our data, and a last one to export the data. We will start by analyzing the block responsible of the table creation.\nFilename: data_exporters/create_duckdb_table.py\n\n1import os\n2import duckdb\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n# -- snip --\n\n\n1\n\nWe will use the os module in our tests.\n\n2\n\nPython DuckDB client.\n\n\n\n\nWe start with the imports‚Äîof course, we‚Äôll need the DuckDB client. If you‚Äôve been following this tutorial, you should have installed it along with the dependencies. Next, we define a query to create the table where we will store our data. This step is crucial to specify the data types for each column.\nAnother critical aspect is defining constraints for the table. Here, we specify that two columns cannot contain null values and designate invoice_line_no as the primary key. But is this necessary? If we aim to enforce data integrity and prevent duplication, the answer is yes.\nFilename: data_exporters/create_duckdb_table.py\n\n# -- snip --\n\ncreate_table_query = \"\"\"\n    CREATE TABLE IF NOT EXISTS iowa_liquor_sales (\n        invoice_line_no TEXT PRIMARY KEY NOT NULL,\n        date TIMESTAMP NOT NULL,\n        store TEXT,\n        name TEXT,\n        address TEXT,\n        city TEXT,\n        zipcode TEXT,\n        store_location TEXT,\n        county_number TEXT,\n        county TEXT,\n        category TEXT,\n        category_name TEXT,\n        vendor_no TEXT,\n        vendor_name TEXT,\n        itemno TEXT,\n        im_desc TEXT,\n        pack REAL,\n        bottle_volume_ml REAL,\n        state_bottle_cost REAL,\n        state_bottle_retail REAL,\n        sale_bottles REAL,\n        sale_dollars REAL,\n        sale_liters REAL,\n        sale_gallons REAL,\n        liquor_type TEXT,\n        is_premium BOOLEAN,\n        bottle_size TEXT,\n        gov_profit_margin REAL,\n        gov_retail_markup_percentage REAL,\n        price_per_liter REAL,\n        price_per_gallon REAL,\n        total_volume_ordered_L REAL,\n        volume_to_revenue_ratio REAL,\n        week_day INTEGER,\n        is_weekend BOOLEAN,\n        quarter INTEGER\n);\n\"\"\"\n\n# -- snip --\n\nThis query also helps eliminate potential ambiguities when the data is exported. With the query defined, we can now move on to the main functionality of this block, the exporter13. The logic is quite simple.\nFilename: data_exporters/create_duckdb_table.py\n\n# -- snip --\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n\n1    conn = duckdb.connect(\"data/iowa_liquor.duckdb\")\n2    conn.sql(create_table_query)\n3    conn.close()\n    \n4    return data\n\n# -- snip --\n\n\n1\n\nCreate a connection to a file called data/iowa_liquor.duckdb. If the file doesn‚Äôt exist it should create it\n\n2\n\nCreates the table with our query.\n\n3\n\nExplicitly close the connection so no access to the database is blocked downstream\n\n4\n\nReturn the unmodified data that was passed upstream.\n\n\n\n\nHere, we connect to our local database, create the table if it doesn‚Äôt exist, and close the connection to avoid blocking its access downstream. And that‚Äôs the main task of this block. Then, we validate that untouched upstream data has been passed and that the database file is also there:\nFilename: data_exporters/create_duckdb_table.py\n\n# -- snip --\n\n@test\ndef test_output(output, *args) -&gt; None:\n    assert output is not None, 'The output is undefined'\n\n@test\ndef db_exist(*args) -&gt; None:\n    assert os.path.exists(\"data/iowa_liquor.duckdb\"), \"The database file doesnt exist\"\n\nNow we move to the real exporter, and the last block of our flow. And as you already guessed, here is where we store the retrieved data to our local database. First we establish a connection with our database, and use the object conn to interact with it, then we try to insert the freshly fetched and transformed data to the table iowa_liquor_sales. If a constraint exception is raised we check for duplicates in our batch, exclude them, and try to insert them again. This exception handling will be specially useful when we are at the stage of just updating with the last records our data base. You can examine this in the block:\nFilename: data_exporters/export_polars_to_duckdb.py\n\n# -- snip --\n\n@data_exporter\ndef insert_data_in_table(data, *args, **kwargs):\n1    conn = duckdb.connect(\"data/iowa_liquor.duckdb\")\n\n2    try:\n        conn.register(\"data\", data)\n        conn.execute(\"INSERT INTO iowa_liquor_sales SELECT * FROM data\")\n3    except duckdb.ConstraintException as e:\n        print(e)\n4        existing_keys_df = conn.execute(\"SELECT invoice_line_no FROM iowa_liquor_sales\").fetchdf()\n        existing_keys_series = pl.DataFrame(existing_keys_df)[\"invoice_line_no\"] \n\n        filtered_data = data.filter(~data[\"invoice_line_no\"].is_in(existing_keys_series))\n\n5        if filtered_data.height &gt; 0:\n            conn.register(\"filtered_data\", filtered_data)  \n            conn.execute(\"INSERT INTO iowa_liquor_sales SELECT * FROM filtered_data\")\n        else:\n            print(\"No new records to insert.\")\n\n6    conn.close()\n\n    print(\"Data loaded to your DuckDB database!\")\n\n\n1\n\nConnect to the local database.\n\n2\n\nAttempt to insert the data into the iowa_liquor_sales table.\n\n3\n\nIf a constraint exception occurs, it likely means the batch of data pulled contains records already present in the database. It should not be about null values since we have already validated that upstream.\n\n4\n\nIn case of an exception, query the invoice IDs from the local database and filter out the duplicate records from the incoming batch.\n\n5\n\nIf the filtered DataFrame still contains records, proceed with inserting them into the table.\n\n6\n\nClose the database connection to ensure safety and prevent potential issues.\n\n\n\n\nWe have finished the implementation of our pipeline! Congrats!\n\nTest the pipeline in Mage\nNow it‚Äôs time to see if it works. To do this, click on the three-dot icon in the upper-right corner of your last block (export_polars_to_duckdb).\n\nHere, select the option labeled ‚ÄúExecute with all upstream blocks.‚Äù This will run the entire pipeline. If an error occurs, Python/Mage will notify you, allowing you to begin debugging. If the process runs successfully, then your first batch of data will be stored in your local database."
  },
  {
    "objectID": "data_pipeline_mage.html#run-the-data-pipeline-in-the-terminal",
    "href": "data_pipeline_mage.html#run-the-data-pipeline-in-the-terminal",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Run the data pipeline in the terminal",
    "text": "Run the data pipeline in the terminal\nNow open a terminal in the project folder and run:\n\nmage run . socrata_iowa_liquor_pipeline\n\nWe use the run command to execute the pipeline. The dot (.) refers to the current directory (assuming you‚Äôre in the project‚Äôs folder), and socrata_iowa_liquor_pipeline is the name of our pipeline. If everything runs successfully, your second batch should now be stored in your local database. A successful output will look like this:\n\nuser@user-pc:~/mage_duckdb_pipeline$ mage run . socrata_iowa_liquor_pipeline\nFetching the records-per-year metadata. This might take a couple minutes..\nDone! We have our year record metadata.\n\nSODA data pull started.\nYears to be fetched: 2017, 2018, 2019, 2020, 2021.\nFetching data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [10:08&lt;00:00, 121.69s/it]\nProduct-related new variables, generated.\nSales and price related metrics, computed.\nVolume-based features, computed.\nTime-based features, computed.\nData loaded to your DuckDB database!\nPipeline run completed.\n\nYou can continue with this until your local data is up to date. Remember that we set a limit to fetch a maximum of five years per run. Feel free to adjust this in the pull_data_from_socrata loader.\n\nShould I manually run the pipeline whenever I need to update the data?\nRunning the pipeline in your terminal might be an option, but not the most appealing one, depending on your use case. For instance, the Iowa Liquor data set is updated once per month with the new data available on the 1st of each month, according to its documentation. Therefore, it makes sense to run an update job once per month. We can configure a trigger for this, and it is very easy to do, so go review Mage‚Äôs documentation if you are in the need.\n\n\n\nProgress we‚Äôve made so far.\n\n\nAnd our data is ready to be served via SQL! We can continue with the pipeline‚Äôs end goal: analytics and predictive modeling."
  },
  {
    "objectID": "data_pipeline_mage.html#data-analytics",
    "href": "data_pipeline_mage.html#data-analytics",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Data analytics",
    "text": "Data analytics\nMany proprietary dashboarding tools can take advantage of our DuckDB database. But in keeping with the spirit of our project, we will use the R language for this purpose since it is free, open-source14, and extremely powerful.\nWe start by connecting to our local database within our R environment. This connection will be called conn:\n\nconn &lt;- DBI::dbConnect(\n  drv = duckdb::duckdb(),\n  dbdir = \"../data/iowa_liquor.duckdb\",\n  read_only = TRUE\n)\n\nNotice that we indicate read_only = TRUE so this connection can be shared between processes. With our connection in place, let‚Äôs query some data to visualize:\n\nSELECT \n  date,\n  liquor_type,\n  SUM(sale_bottles) AS bottles_sold\nFROM iowa_liquor_sales\nGROUP BY date, liquor_type\n\nThe output table of this query is stored in the object bottles_sold_per_type. Let‚Äôs have a peek at it:\n\nbottles_sold_per_type %&gt;% \n  head(5) %&gt;% \n  gt::gt()\n\n\n\n\n\n\n\ndate\nliquor_type\nbottles_sold\n\n\n\n\n2013-02-21\nVodka\n27282\n\n\n2013-09-17\nSchnapps\n2647\n\n\n2013-06-15\nVodka\n22027\n\n\n2013-02-04\nSchnapps\n4297\n\n\n2013-12-16\nBrandy\n6177\n\n\n\n\n\n\n\nLet‚Äôs say we want to know how well we sell liquor and see if there is a trend in our historical data. We start by aggregating our freshly queried data. One advantage of using R is that its libraries have pretty useful abstractions; for example, we will use floor_date from lubridate to aggregate the data by month.\n\nbottle_sales_by_month &lt;- bottles_sold_per_type %&gt;% \n  group_by(month = floor_date(date, 'month'), # Group by month\n           liquor_type) %&gt;% \n  summarise(monthly_bottles_sold = sum(bottles_sold))\n\nUsing lubridate is more straightforward and less verbose than applying this logic in SQL, so we took advantage of R capabilities. Now that our data is ready let‚Äôs visualize it to get some knowledge:\n\nmonth_sales_plot &lt;- ggplot(bottle_sales_by_month, aes(x = month, y = monthly_bottles_sold, color = liquor_type)) +\n  geom_line() + \n  geom_point() +\n  labs(\n    title = \"Liquor bottles sold by month\",\n    x = \"Date\",\n    y = \"Bottles sold\",\n    color = \"Liquor type\"\n  ) +\n  theme_minimal() + \n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n\nplotly::ggplotly(month_sales_plot)\n\n\n\n\n\nSince this chart is interactive, you can zoom in to examine it further; to zoom out, double-click on the chart.\nThe most popular liquors in Iowa seem to be whisky and vodka. The good news for Mezcal adepts is that it has been available in stores since 2016. Let‚Äôs say we like mezcal and want to buy a bottle as a treat. However, as the chart shows, it is not the most popular drink, so now we want to know where to buy a bottle. Our quest will begin with a query to our DuckDB.\nFor this query, we will use a Common Table Expression (CTE) to retrieve the latest record for each store. We also filter the data to include only records from 2024 onward, where a geolocation is available, and the invoice corresponds to a mezcal purchase. Since the CTE ranks records by the most recent purchases, we can easily select the first record for each store in the main query.\n\nWITH ranked_stores AS (\n    SELECT \n      store, \n      name, \n      address, \n      store_location,\n      ROW_NUMBER() OVER (PARTITION BY store ORDER BY date DESC) AS row_num\n    FROM iowa_liquor_sales\n    WHERE YEAR(date) &gt;= 2024\n          AND store_location IS NOT NULL\n          AND liquor_type = 'Mezcal'\n)\n\nSELECT store, name, address, store_location\nFROM ranked_stores\nWHERE row_num = 1;\n\nThe resulting table is stored in bottles_sold_per_type, let‚Äôs examine it:\n\nbottles_sold_per_type %&gt;% \n  head(3) %&gt;% \n  gt::gt()\n\n\n\n\n\n\n\nstore\nname\naddress\nstore_location\n\n\n\n\n010065\nMADELLA WINE AND SPIRITS / BETTENDORF\n3263 MOENCKS COURT UNIT 3265\nPOINT (-90.44919 41.55596)\n\n\n010076\nHOMETOWN SPIRITS / TIFFIN\n119 WEST MARENGO ROAD\nPOINT (-91.664284585 41.70668719)\n\n\n010121\nBIG 10 MART #18 / DAVENPORT\n5310 CORPORATE PARK DRIVE\nPOINT (-90.517948014 41.575410995)\n\n\n\n\n\n\n\nIt contains the:\n\nStore code (ID)\nStore address\nStore name\nGeolocation\n\nAll the information we need to get our bottle. But a table with more than 300 stores is not easy to digest. So let‚Äôs put that into a map; locating the stores closest to us would be easy.\nWe will begin by converting our retrieved information into a simple features object (sf). The sf library greatly simplifies geospatial analysis, especially when working with geometry data types. First, we transform the store_location column (which is string type) into a POINT geometry type using the st_as_sfc function. Specifying the coordinate reference system (crs = 4326) is crucial here. I recommend revisiting Lovelace, Nowosad, and Muenchow (2025) to learn more about simple features and geospatial analysis. I learned a lot in the 1st edition, and this second one is even better!\nAlthough our dataset documentation does not explicitly mention the coordinate reference system, WGS84 CRS (EPSG:4326) is the most logical choice. This is because WGS84 is the standard and most widely used system for representing latitude-longitude coordinates, making it the most likely CRS for our data15.\n\nbottles_sold_per_type_0 &lt;- bottles_sold_per_type %&gt;%\n  mutate(geometry = st_as_sfc(store_location, crs = 4326)) %&gt;% \n  select(-store_location) %&gt;% \n  st_as_sf()\n\n# To peep the contents of our sf object\nbottles_sold_per_type_0\n\nSimple feature collection with 343 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -96.43086 ymin: 40.40001 xmax: -90.1915 ymax: 43.44369\nGeodetic CRS:  WGS 84\nFirst 10 features:\n    store                                    name                      address\n1  010065   MADELLA WINE AND SPIRITS / BETTENDORF 3263 MOENCKS COURT UNIT 3265\n2  010076               HOMETOWN SPIRITS / TIFFIN        119 WEST MARENGO ROAD\n3  010121             BIG 10 MART #18 / DAVENPORT    5310 CORPORATE PARK DRIVE\n4  010153               IOWA SPIRITS / BURLINGTON           2107 AGENCY STREET\n5  010218       ABARROTES GASCA LLC / ROCK VALLEY           954 WESTVIEW DRIVE\n6  010304 VINTON LIQUOR TOBACCO AND VAPE / VINTON                 411 A AVENUE\n7  010320                    SUPER MART / OELWEIN   701 SOUTH FREDERICK AVENUE\n8    2535       HY-VEE FOOD STORE #1 (1887) / WDM         1700 VALLEY WEST DR.\n9    2576    HY-VEE WINE AND SPIRITS / STORM LAKE               1250 N LAKE ST\n10   2588  HY-VEE FOOD AND DRUG #6 / CEDAR RAPIDS         4035 MT VERNON RD SE\n                     geometry\n1  POINT (-90.44919 41.55596)\n2  POINT (-91.66428 41.70669)\n3  POINT (-90.51795 41.57541)\n4  POINT (-91.12961 40.81423)\n5  POINT (-96.30885 43.19936)\n6  POINT (-92.02559 42.16706)\n7  POINT (-91.91348 42.66924)\n8  POINT (-93.75334 41.59769)\n9    POINT (-95.2033 42.6533)\n10 POINT (-91.60966 41.97553)\n\n\nWe displayed a print of the contents of our sf object so you can get a sense of what it is: a data frame with geospatial metadata and geometry types. DuckDB has some extensions that make geospatial types available, but we are not using them here to avoid adding more complexity to the upstream system.\nLet‚Äôs continue with our map. We will create the map using the leaflet library; it is easy to use and has beautiful and usable results. We feed this visualization with the sf object we just created.\n\nleaflet(data = bottles_sold_per_type_0) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%  # Add a base map\n  addCircleMarkers(\n    lng = ~st_coordinates(geometry)[, 1],  # Extract longitude\n    lat = ~st_coordinates(geometry)[, 2],  # Extract latitude\n    popup = ~paste0(\n      \"&lt;strong&gt;Store:&lt;/strong&gt; \", name, \"&lt;br&gt;\",\n      \"&lt;strong&gt;Address:&lt;/strong&gt; \", address, \"&lt;br&gt;\"\n    ),\n    radius = 5,\n    color = \"#ed7c65\",\n    fillOpacity = 0.7\n  ) %&gt;%\n  addLegend(\n    position = \"bottomright\",\n    title = \"Liquor stores that sell mezcal in Iowa\",\n    colors = \"#ed7c65\",\n    labels = \"Store locations\",\n    opacity = 0.7\n  )\n\n\n\n\n\nGreat! Now, we have an interactive map of the stores that recently ordered mezcal. We can look for the closest one and go there for our treat. You can also click the dot of your store of interest to get its name and address.\n\n\n\nProgress we‚Äôve made so far.\n\n\nNow that we better understand our data, we can use our DuckDB to feed some algorithms and get predictive (machine learning) models."
  },
  {
    "objectID": "data_pipeline_mage.html#predictive-modeling-machine-learning",
    "href": "data_pipeline_mage.html#predictive-modeling-machine-learning",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Predictive modeling (machine learning)",
    "text": "Predictive modeling (machine learning)\nWe have a lightning-fast local database for advanced analytics, and we can use the same resource for statistical and predictive modeling. Let‚Äôs say some gangster is dissin ya‚Äô fly girl we‚Äôre in the middle of 2019, and the supply chain team is interested in rum supply for the end of the year due to the upcoming holidays.\n\n\n\n\n\nYou don‚Äôt know the details well, but you suspect it has something to do with a conversation you overheard about Martha Stewart‚Äôs jet fuel recipe getting too popular. Anyways, you‚Äôre excited since you were looking forward to doing some forecasting work by now.\nYou start by setting up your Python environment for this job. R is undoubtedly nice for advanced analytics, but you do your machine learning with Python16.\n\nimport duckdb\nimport pandas as pd\nfrom sktime.forecasting.compose import make_reduction\nfrom sktime.forecasting.conformal import ConformalIntervals  \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nimport matplotlib.pyplot as plt\n\nWith your prelude ready, we continue by establishing a connection with our local database:\n\nconn_py = duckdb.connect(\"../data/iowa_liquor.duckdb\", read_only=True)\n\nAfter we have created a connection in Python, we can query the data we need for the model.\n\nquery_liquors_sales = \"\"\"\n  SELECT \n      DATE_TRUNC('month', date) AS year_month,\n      MONTHNAME(year_month) AS month,\n      liquor_type,\n      SUM(sale_bottles) AS bottles_sold\n  FROM iowa_liquor_sales\n  WHERE YEAR(date) &lt; 2020\n  GROUP BY year_month, liquor_type\n  ORDER BY year_month;\n\"\"\"\n\ndata_df = conn_py.sql(query_liquors_sales).df()\n\ndata_df.head(5)\n\n  year_month    month liquor_type  bottles_sold\n0 2012-01-01  January    Schnapps       83489.0\n1 2012-01-01  January         Gin       46827.0\n2 2012-01-01  January     Tequila       51814.0\n3 2012-01-01  January       Vodka      365226.0\n4 2012-01-01  January       Cream       24192.0\n\n\nWe write the query as a docstring and then pass it to the .sql() command. Also, notice that we get our query results as a Pandas data frame with the .df() at the end.\nSince we are working with time series data, we will need to reshape our data.\n\npivoted_data = data_df.pivot(index   = ['year_month', 'month'], \n                             columns = 'liquor_type', \n                             values  = 'bottles_sold').reset_index()\n\n# Fill missing values\npivoted_data = pivoted_data.fillna(0)\n\npivoted_data.head(5)\n\nliquor_type year_month     month   Brandy  ...  Tequila     Vodka    Whisky\n0           2012-01-01   January  71967.0  ...  51814.0  365226.0  340421.0\n1           2012-02-01  February  83090.0  ...  60387.0  377666.0  431452.0\n2           2012-03-01     March  81131.0  ...  61047.0  412089.0  373689.0\n3           2012-04-01     April  83601.0  ...  79944.0  443737.0  402901.0\n4           2012-05-01       May  87468.0  ...  84798.0  501372.0  468374.0\n\n[5 rows x 12 columns]\n\n\nHere, we pivoted in a wider shape our base data frame and created a new time series column for each liquor type, leaving in place the columns year_month and month. We also fill the missing values with zeroes. For example, remember that we started recording official mezcal sales in 2016, meaning that the NaN at the beginning of the Mezcal column will be filled with zeroes. This is because we will use the sales of each type of liquor as predictors of rum sales.\nNext, we will organize our data according to the time series structure. Remember that the order of our observations matters in this kind of data. Also we will split our data in two sets, one for training, another for testing.\n\n1pivoted_data['year_month'] = pd.to_datetime(pivoted_data['year_month'])\npivoted_data = pivoted_data.sort_values(by='year_month')\n\n# Define target (Rum) and features (all other columns except year_month and Rum)\n2target   = 'Rum'\nfeatures = [col for col in pivoted_data.columns if col not in ['year_month', 'Rum']]\n\n# Separate target and features\n3y = pivoted_data[target]\nX = pivoted_data[features]\n\n# Define test period (last two quarters of 2019)\n4train_cutoff = pivoted_data['year_month'] &lt; '2019-07-01'\ntest_cutoff  = ~train_cutoff\n\n5X_train, X_test = X[train_cutoff], X[test_cutoff]\ny_train, y_test = y[train_cutoff], y[test_cutoff]\n\n\n1\n\nEnsure the data is in chronological order.\n\n2\n\nIdentify the target variable and the features.\n\n3\n\nSeparate the target variable from the features.\n\n4\n\nDefine a test period, which in this case will be the last two quarters of 2019. The train_cutoff and test_cutoff objects are boolean Pandas series used to filter the data.\n\n5\n\nSplit the data into training and testing datasets.\n\n\n\n\nWith our disjoint sets ready, we can continue with the next step, encoding our categorical month variable using the one-hot encoding method.\n\n1encoder = OneHotEncoder(sparse_output=False, drop='first')\nencoded_months_train = encoder.fit_transform(X_train[['month']])\nencoded_months_test  = encoder.transform(X_test[['month']])\n\n2month_columns   = encoder.get_feature_names_out(['month'])\nX_train_encoded = pd.DataFrame(encoded_months_train, columns=month_columns, index=X_train.index)\nX_test_encoded  = pd.DataFrame(encoded_months_test, columns=month_columns, index=X_test.index)\n\n# Drop the original 'month' column and add the encoded columns\n3X_train = X_train.drop(columns=['month']).join(X_train_encoded)\nX_test  = X_test.drop(columns=['month']).join(X_test_encoded)\n\n\n1\n\nApply one-hot encoding to the month variable.\n\n2\n\nReplace the original month column with the one-hot encoded columns.\n\n3\n\nRemove the original month column and integrate the encoded columns into the dataset.\n\n\n\n\nWith the months now encoded in our feature sets (X_), we are ready to train the model. Since this is a forecasting problem, additional steps are required to adapt our regression model. We will use the make_reduction function from sktime, a machine learning framework for time series, to transform a simple estimator‚ÄîRandomForestRegressor in this case‚Äîinto a forecaster capable of handling time series data.\n\n# Define the forecaster\nforecaster = make_reduction(\n    RandomForestRegressor(random_state=42), # Scikit-learn regression model\n    strategy=\"recursive\", \n    window_length=6  # Use 6 months of past data\n)\n\n# Wrap the forecaster with ConformalIntervals\nconformal_forecaster = ConformalIntervals(forecaster)\n\n# Fit the conformal forecaster\nconformal_forecaster.fit(y_train, X=X_train)\n\nConformalIntervals(forecaster=RecursiveTabularRegressionForecaster(estimator=RandomForestRegressor(random_state=42),\n                                                                   window_length=6))Please rerun this cell to show the HTML repr or trust the notebook.ConformalIntervals?Documentation for ConformalIntervalsConformalIntervals(forecaster=RecursiveTabularRegressionForecaster(estimator=RandomForestRegressor(random_state=42),\n                                                                   window_length=6))forecaster: RecursiveTabularRegressionForecasterRecursiveTabularRegressionForecaster(estimator=RandomForestRegressor(random_state=42),\n                                     window_length=6)estimator: RandomForestRegressorRandomForestRegressor(random_state=42)RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\nFirst, we instantiate the forecaster, which uses a RandomForestRegressor as its core model. Next, we wrap it in the ConformalIntervals class to create a conformal predictor. This allows us not only to make predictions but also to generate confidence intervals, adding an extra layer of uncertainty quantification to our forecasts. The hierarchical structure of the model can be inspected in the visualization displayed below the preceding code snippet.\nOnce the model is set up, we train it using the fit function. With the trained model ready, we can proceed to forecasting. Now, let‚Äôs predict the rum sales:\n\n# Define the forecasting horizon\nfh = list(range(1, len(y_test) + 1))  # Forecast horizon matches test data length\n\n# Point forecasts\ny_pred = conformal_forecaster.predict(fh=fh, X=X_test)\n\n# Predict confidence intervals\npred_int = conformal_forecaster.predict_interval(fh=fh, X=X_test, coverage=0.9)\n\nForecasted rum sales for the last two quarters of 2019:\n\nprint(y_pred)\n\n90    223255.97\n91    214242.75\n92    214532.89\n93    228206.81\n94    218049.97\n95    222827.08\nName: Rum, dtype: float64\n\n\nForecasted rum sales confidence interval (90% coverage) for the last two quarters of 2019:\n\nprint(pred_int)\n\n            Rum             \n            0.9             \n          lower        upper\n90   181568.086    261905.29\n91   167177.517  252442.6645\n92   168367.598   254874.554\n93    188172.32   264794.065\n94   180995.268   254576.388\n95  183766.0995  262251.6415\n\n\nHere is the predicted output for the last two 2019 quarters. Let‚Äôs evaluate the model to see how well it performed against the observed ground truth.\n\n# Evaluate the forecast\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\n\nprint(f\"Mean Absolute Error (MAE): {mae}\")\n\nMean Absolute Error (MAE): 15972.041666666672\n\nprint(f\"Mean Squared Error (MSE): {mse}\")\n\nMean Squared Error (MSE): 307287238.5831502\n\n\nThese numbers don‚Äôt tell us much on their own, and it would be ideal to compare them against the MAE and MSE of another model. However, we won‚Äôt invest more time in that. Feel free to train a different model to make the comparison!\nNext, a key step in evaluating the model is visualizing how its forecasts compare with the observed ground truth. To achieve this, we create a plot of our forecast:\n\n# Extract CI\ny_pred_lower = pred_int[('Rum', 0.9, 'lower')].values\ny_pred_upper = pred_int[('Rum', 0.9, 'upper')].values\n\n# Combine actual and predicted data\nresults_2019 = pivoted_data[pivoted_data['year_month'].dt.year == 2019][['year_month', 'Rum']].copy()\nresults_2019['Forecast'] = None\nresults_2019['Lower'] = None\nresults_2019['Upper'] = None\n\n# Assign forecast values and prediction intervals for the test period\ntest_period = results_2019['year_month'] &gt;= '2019-07-01'\nresults_2019.loc[test_period, 'Forecast'] = y_pred.values\nresults_2019.loc[test_period, 'Lower'] = y_pred_lower\nresults_2019.loc[test_period, 'Upper'] = y_pred_upper\n\n# Create figure\nplt.figure(figsize=(10, 6))\n\n# Plot actual Rum sales\nplt.plot(results_2019['year_month'], results_2019['Rum'], label='Actual rum sales', marker='o')\n\n# Plot forecasted Rum sales\nplt.plot(results_2019['year_month'], results_2019['Forecast'], label='Forecasted rum sales', linestyle='--', marker='x', color='#b1d25a')\n\n# Plot prediction intervals as separate lines\nplt.plot(results_2019.loc[test_period, 'year_month'], results_2019.loc[test_period, 'Lower'], label='Lower bound', linestyle=':', color='#b1d25a')\nplt.plot(results_2019.loc[test_period, 'year_month'], results_2019.loc[test_period, 'Upper'], label='Upper bound', linestyle=':', color='#b1d25a')\n\n# Highlight the test period\nplt.axvline(x=pd.to_datetime('2019-07-01'), color='#ed7c65', linestyle='--', label='Start of forecast period')\n\nplt.title('Forecast of rum bottles sold for 2019 Q3 and Q4')\nplt.xlabel('Month')\nplt.ylabel('Bottles sold')\nplt.xticks(rotation=45)\n\n(array([17897., 17956., 18017., 18078., 18140., 18201.]), [Text(17897.0, 0, '2019-01'), Text(17956.0, 0, '2019-03'), Text(18017.0, 0, '2019-05'), Text(18078.0, 0, '2019-07'), Text(18140.0, 0, '2019-09'), Text(18201.0, 0, '2019-11')])\n\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe see that the ground truth falls within our confidence interval; we can also compare our prediction with the ground truth (consider that the ground truth won‚Äôt be present in real life since that period hasn‚Äôt happened). So you can now go with your stakeholders and present this chart so they can organize their rum orders for that year‚Äôs holiday season.\nThat‚Äôs fantastic! But can we go even further downstream in our data flow by diving into stakeholder management, effective result presentation, and model monitoring? ü§©\nYes, but ‚Äúwhat do we say to the god of death?‚Äù\n\n\n\n\n\nAnd there you have it: we have used our DuckDB data to create a predictive model!\n\n\n\nProgress we‚Äôve made so far.\n\n\nAt this point, you must be overwhelmed and tired, but that‚Äôs because we have worked and implemented a whole data life cycle. Congratulation!"
  },
  {
    "objectID": "data_pipeline_mage.html#recap",
    "href": "data_pipeline_mage.html#recap",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Recap",
    "text": "Recap\nDuring this project, we did a lot. We started analyzing the source of our data and thinking about the best way to retrieve it. We coded a mechanism to fetch the data, transformed it with Polars, and then stored it on disk. Finally, we illustrated that the data we got is indeed helpful in answering real-life questions.\nThe data landscape is vast, with many appliances available for every task. In this project, we explored several modern tools to address a data engineering problem. However, it‚Äôs important not to limit yourself to these specific libraries. Each issue may require a different set of tools, but the key is to focus on the foundational principles of data engineering. The technical stack can be chosen later, but the core concepts will pave the way to finding practical solutions to your challenges.\nWithout more, I hope here you have learned a little and that you can apply some of these things in your tasks :)"
  },
  {
    "objectID": "data_pipeline_mage.html#final-remarks",
    "href": "data_pipeline_mage.html#final-remarks",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Final remarks",
    "text": "Final remarks"
  },
  {
    "objectID": "data_pipeline_mage.html#what-was-left",
    "href": "data_pipeline_mage.html#what-was-left",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "What was left",
    "text": "What was left"
  },
  {
    "objectID": "data_pipeline_mage.html#citation",
    "href": "data_pipeline_mage.html#citation",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Citation",
    "text": "Citation\nNo citation is required, but if you found this project helpful, I‚Äôd greatly appreciate a mention! üòÑ"
  },
  {
    "objectID": "data_pipeline_mage.html#acknowledgements",
    "href": "data_pipeline_mage.html#acknowledgements",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nTo Little Mai, my home office manager and editor-in-chief of ‚ÄúPaws-On Data.‚Äù\n\n\n\nLittle Mai"
  },
  {
    "objectID": "data_pipeline_mage.html#contact",
    "href": "data_pipeline_mage.html#contact",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Contact",
    "text": "Contact\nHave questions, suggestions, or just want to connect? Feel free to reach out!\n\nLinkedIn: Jos√© Pablo Barrantes\nBlueSky: doggofan77.bsky.social"
  }
]