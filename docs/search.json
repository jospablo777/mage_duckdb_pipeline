[
  {
    "objectID": "data_pipeline_mage.html#where-are-we-in-the-data-lifecycle",
    "href": "data_pipeline_mage.html#where-are-we-in-the-data-lifecycle",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Where are we in the data lifecycle?",
    "text": "Where are we in the data lifecycle?\nAs data analysts or scientists, we often find ourselves working downstream in the data lifecycle. Most of the time, our role involves transforming and analyzing data that has already been prepared and served to us by upstream processes. However, having a deeper understanding of the entire data pipeline‚Äîfrom ingestion to transformation and storage‚Äîcan empower us to optimize workflows, ensure data quality, and unlock new insights.\n\n\n\nThe data lifecycle and where you are. Modified from ‚ÄúFundamentals of Data Engineering‚Äù by Reis & Housley (2022).\n\n\nAnother advantage of gaining understanding‚Äîand hands-on experience‚Äîin data engineering processes is the empathy we build with our data engineers. These are the colleagues we work closely with and rely on, making a strong, collaborative relationship essential. Hence, in this hands-on article, we will explore the Python ecosystem by examining tools such as Mage, Polars, and DuckDB. We‚Äôll demonstrate how these tools can help us build efficient, lightweight data pipelines that take data from the source and store it in a format that is well-suited for high-performance analytics."
  },
  {
    "objectID": "data_pipeline_mage.html#about-the-tools",
    "href": "data_pipeline_mage.html#about-the-tools",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "About the tools",
    "text": "About the tools\nIn this project, we will use Mage as our data orchestrator. The tasks managed by the orchestrator will include data manipulation and transformation using Polars, as well as persistent storage with DuckDB.\n\nWhat are data orchestrators?\nData orchestrators, such as Mage, are tools that help manage, schedule, and monitor workflows in data pipelines. They allow us to automate complex processes, ensuring that tasks are executed in the correct order and dependencies are handled seamlessly. By using Mage as our orchestrator, we can streamline our data pipeline and focus on building efficient workflows that allow us to set a local database appropriate for our analysis.\n\n\nData manipulation and storage\nWhy Polars and DuckDB? The answer lies in the size and nature of the dataset we‚Äôre working with. Since our dataset is small enough to fit in memory, we don‚Äôt need a distributed system like Spark. Polars, with its fast and memory-efficient operations, is perfect for data manipulation and transformation. Meanwhile, DuckDB provides a lightweight, yet powerful, SQL-based engine for persistent storage and querying. Together, these tools offer a simple, performant, and highly efficient solution for handling our data pipeline.\nReturning to the data lifecycle diagram, we can land it in a more concrete way to show how our project will be built and executed:\n\n\n\nThe data lifecycle and a more concrete overview of what we will implement in this project. Modified from ‚ÄúFundamentals of Data Engineering‚Äù by Reis & Housley (2022).\n\n\nFirst, we will fetch the data from the Socrata Open Data API (SODA) through HTTP. After that, we will generate some extra variables of our interest with Polars, to finally store it in a DuckDB database we will consume for analytics and predictive modeling. All this is orchestrated with Mage.\n\n\nTools and resources overview:\n\nSODA API: provides access to open datasets, serving as our data source. It contains ‚Äúa wealth of open data resources from governments, non-profits, and NGOs around the world1.‚Äù\nMage: the data orchestrator that will automate and manage the pipeline.\nPolars: a high-performance data frame library implemented in Rust, ideal for fast data manipulation.\nDuckDB: a columnar database system designed for efficient analytics and in-memory processing.\n\nWe‚Äôve chosen the Iowa Liquor Sales dataset since it is big enough to make this ETL (extract, transform, load) pipeline interesting."
  },
  {
    "objectID": "data_pipeline_mage.html#our-problem",
    "href": "data_pipeline_mage.html#our-problem",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Our problem",
    "text": "Our problem\nLet‚Äôs say we work for a big chain of liquor stores in the US, Iowa. Part of the intelligence in your company is built upon the information made available through SODA API, and it feeds some of the dashboards the decision-makers consume. You also use it often to do research and predictive modeling. Some stakeholders have started complaining about the loading times of the dashboards, and you have also been a little frustrated with the time it takes to get the data to train your predictive models.\n\nTo continue building our situation, let‚Äôs imagine the year 2020‚Äîwhen the term ‚Äúdata engineer‚Äù was not as popular as it is today. You‚Äôve just been hired as a data analyst2, and according to Google Trends, the term ‚Äúdata engineer‚Äù was only half as popular as it is now. Moreover, a closer look at the trend data from 2020 reveals that most searches for this term originated in tech hubs like California or Washington rather than in states like Iowa.\n\n\n\n\n\nGoogle trends for ‚ÄòData Engineering‚Äô in the US, starting from 2004 to the date (Dec 2024)\n\n\n\n\nAt this point, you know you are on your own to optimize this process3. And you already have a clear outline for this process:\n\nPull the data from the API.\nGenerate the variables that provide the most valuable insights for the teamÔ∏è.\nStore the data in a location that allows for easy and fast retrieval .\n\nThis brings us to our current challenge: finding a more agile and efficient way to make the SODA liquor sales data accessible to the rest of the company."
  },
  {
    "objectID": "data_pipeline_mage.html#solution-implementation-getting-started",
    "href": "data_pipeline_mage.html#solution-implementation-getting-started",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Solution implementation: getting started",
    "text": "Solution implementation: getting started\nAs outlined earlier, the first step involves pulling the data from the API. However, before that, we need to set up Mage and configure the rest of our environment. To begin, we‚Äôll clone the Git project and navigate to the project directory:\n\ngit clone https://github.com/jospablo777/mage_duckdb_pipeline.git\ncd mage_duckdb_pipeline\n\nNext, we‚Äôll set up a virtual environment and install the necessary libraries:\n\npython -m venv venv             # The first 'venv' is the command, the second is the name of the folder for the virtual environment.\nsource venv/bin/activate        # Activate the virtual environment.\npip install -r requirements.txt # Install dependencies from the requirements file.\n\nThe requirements.txt file contains the libraries required for the project, with the key players being mage-ai and duckdb. Now, that we have all our dependencies ready we can start our Mage project with:\n\nmage start\n\nThis will open a tab in our browser that looks like this:\n\n\n\nMage home UI.\n\n\nOne of Mage‚Äôs greatest strengths is its intuitive user interface. It allows us to easily create and manage data pipelines. In this example, we‚Äôve named our pipeline socrata_iowa_liquor_pipeline. Once the pipeline is created, we can navigate to it using the left panel, then go to the Pipelines section, where our newly created pipeline will be listed, click on it.\n\n\n\nMage home UI left panel.\n\n\nAfter opening the pipeline, navigate to the Edit pipeline section in the left panel, identified by the &lt;/&gt; symbol. This is where we can begin constructing our pipeline. Here, you will have the option to insert a block:\n\n\n\nCreate block options.\n\n\nOur objective is to build the following pipeline using a series of different blocks:\n\n\n\nPipeline we will implement in this projec.\n\n\nGreat! Now that our environment is set up and we‚Äôve familiarized ourselves with Mage‚Äôs user interface, we‚Äôll return to Mage shortly. But first, let‚Äôs take a closer look at our data source: the Iowa Liquor Sales dataset."
  },
  {
    "objectID": "data_pipeline_mage.html#about-the-polar-bear-in-the-header",
    "href": "data_pipeline_mage.html#about-the-polar-bear-in-the-header",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "About the polar bear in the header",
    "text": "About the polar bear in the header\nhttps://knowyourmeme.com/memes/bonjour-bear"
  },
  {
    "objectID": "data_pipeline_mage.html#cited-works-and-recommended-readings",
    "href": "data_pipeline_mage.html#cited-works-and-recommended-readings",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Cited works and recommended readings",
    "text": "Cited works and recommended readings\n\nReis, J., & Housley, M. (2022). Fundamentals of data engineering: Plan and build robust data systems. O‚ÄôReilly Media."
  },
  {
    "objectID": "data_pipeline_mage.html#footnotes",
    "href": "data_pipeline_mage.html#footnotes",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAccording to their web page :)‚Ü©Ô∏é\nThere is no budget for a ‚Äúscientist,‚Äù and at this point in history, there is no such thing as a ‚Äúdata engineer‚Äù in Latin America Iowa.‚Ü©Ô∏é\nPlease pretend that the tools existed at the time. I certainly wished for a data ecosystem like this, and I was in Latin America (Little Mai and I still are, and we‚Äôre loving it ü™áüêë).‚Ü©Ô∏é\nI spent a few days debugging this and initially signaled Mage as the culprit, but in the end, the SODA API was responsible.‚Ü©Ô∏é\nThe process will get frozen at some point with no notifications or feedback.‚Ü©Ô∏é\nLittle Mai and I üêë.‚Ü©Ô∏é"
  },
  {
    "objectID": "data_pipeline_mage.html#soda-api-our-data-source",
    "href": "data_pipeline_mage.html#soda-api-our-data-source",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "SODA API: our data source",
    "text": "SODA API: our data source\nThe Iowa Liquor Sales data is provided by the Iowa government through the Socrata Open Data API (SODA), a platform designed to grant access to open datasets from government agencies, non-profits, and other organizations. This API allows us to programmatically interact with our dataset of interest, enabling us to retrieve data via HTTP requests.\nThe dataset‚Äôs documentation provides the key information needed to access the data: the source domain (data.iowa.gov) and the dataset identifier (m3tr-qhgy). With this information, we can define the endpoint for sending our requests. The documentation also details the available variables and their respective data types. Below is an example of a base endpoint for this dataset:\n\nhttps://data.iowa.gov/resource/m3tr-qhgy.csv\n\nThe dataset documentation also informs us that, to date, it consists of more than 30 million rows. Now that we know the endpoint and the size of the data, we‚Äôre ready to pull the data, right?\nA sensible approach would be to retrieve the data sequentially using the paging method described in the SODA documentation. This method allows us to fetch the data in manageable batches, specified by the $limit parameter, while navigating through the dataset using the $offset parameter. The process is illustrated in the following diagram:\n\n\n\nLater rows of the data can‚Äôt be fetched due to system limits.\n\n\nNaturally, I started with this approach to find a limitation in the API that was not documented. When we get to the point of pulling the records around row 20M, the data loader will get stuck, and no information will be pulled. This might be by design or due to system limitations in which deep paginations can bog the system4, making this an unreliable method to get the data5. Meaning that we will need a different strategy to retrieve the 30M records.\nThis brings us to the next strategy: using SoQL, the query language of the Socrata API. SoQL is quite similar to SQL, with the key difference that its syntax is structured to work within a URL format.\nWe will send an HTTP request to the server to query individual batches of invoices corresponding to a specific year (based on the date variable). We represent this as follows.\n\n\n\nFetching each year individually: an incremental approach. The queries in the blocks are written in SQL for readability.\n\n\nThis approach limits pagination to the number of years in the dataset rather than the total number of records. By fetching data in yearly batches, our requests won‚Äôt get stuck at an offset of 20 million, as each year contains fewer than 3 million records.\nWhy are we spending so much time on this? Understanding our data source‚Äôs quirks, perks, and limitations is crucial because, ultimately, this knowledge will shape how we design our data pipeline. So it‚Äôs worth it to spend some time understanding the source system; it will save us headaches and result in a better design that is easier to maintain from the beginning.\nNow, we have a clearer understanding of the upper stage in our stream, the data source.\n\n\n\nProgress we‚Äôve made so far.\n\n\nWith this in mind, we can now move on to the ingestion step."
  },
  {
    "objectID": "data_pipeline_mage.html#fetching-the-data",
    "href": "data_pipeline_mage.html#fetching-the-data",
    "title": "Building a data pipeline with Mage, Polars, and DuckDB",
    "section": "Fetching the data",
    "text": "Fetching the data\nWe are aware of the endpoint to request the data and some limitations of this API. We know that we cannot fetch the data as it is because, at some point, our pipeline will clog, and we won‚Äôt be able to request more data from the SODA DB.\nDue to this, we will write more complex HTTP requests using SoQL, the SODA query language. SoQL syntax is similar to SQL, so if you are familiar with relational databases, learning will feel intuitive.\nOur base query to fetch the data will look something like this:\nhttps://data.iowa.gov/resource/m3tr-qhgy.csv?$where=date_extract_y(date)=2013&$limit=2000\n\nTeal: the endpoint for the data request.\nBlue: the WHERE clause of the SoQL query, used to filter records for the year 2013. The function date_extract_y() extracts the year from the date column.\nOrange: the LIMIT clause of the SoQL query, used to limit the response to 2000 records. If we don‚Äôt indicate this, the default will be used, which is 1000 records.\n\nNotice that the ? symbol separates the endpoint from the query parameters, and each clause is separated by an & symbol. To indicate the start of a clause, a $ is used.\nThis request will be sent using the GET HTTP method.\nWith this in mind, we can return to Mage. We‚Äôll begin by creating the first three blocks, which will focus on retrieving metadata. The primary purpose of these blocks is to provide the downstream block with the necessary information on what data to fetch from the endpoint and how to fetch it.\n\n\n\n\n\n\nNote\n\n\n\nIn this tutorial, we will adopt the writing style of The Rust Programming Language book, where the file path is shown before each code block. This approach allows you to easily follow the project‚Äôs structure. The complete code for the project is available on the GitHub repo.\n\n\n\nWrinting our first Mage blocks\nHere we will implement our first block, two of them will fetch metadata from the SODA API, and another one will get metadata from our local database (DuckDB). First lets go to our already created pipeline, socrata_iowa_liquor_pipeline. But if you want to follow this tutorial from scratch, you can also create a new pipeline, for example, socrata_iowa_liquor_pipeline_from_scratch, and continue working from there. With the intuitive Mage UI, it will be easy for you to create this new pipeline.\nOur first block:\n\n\n\nCreating our first block.\n\n\nHere, select the first option, Data loader, then navigate to the creation option and select Python as the language, and then API as the source, then name the block get_schema_from_metadata. When created, the block will come with a template like this:\nFilename: data_loaders/get_schema_from_metadata.py\n\n1import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n2@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = ''\n    response = requests.get(url)\n\n    return pd.read_csv(io.StringIO(response.text), sep=',')\n\n\n3@test\ndef test_output(output, *args) -&gt; None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n\n\n1\n\nImports.\n\n2\n\nMain functionality: since this is a data loader I has the @data_loader decorator on top.\n\n3\n\nTest for data validation: accepts the output data of the block. If any of the tests fail, the block execution will also fail.\n\n\n\n\nMage provides a variety of templates designed to guide you and save time. These templates were incredibly helpful when we6 first started building data pipelines. Notices the structure of a block, it is composed by three sections: library imports, a function, and a test. The function handles the main task of the block, while the test ensures that the block‚Äôs main functionality works as intended.\nNext, we‚Äôll modify the template to suit our specific use case. Overwrite the content of your newly created block with the following code, and let‚Äôs proceed to analyze it:\nFilename: data_loaders/get_schema_from_metadata.py\n\nimport io\n1import polars as pl\n2import requests\nif 'data_loader' not in globals():\n3    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n4    from mage_ai.data_preparation.decorators import test\n\n# Map API data types to Polars types\n5SODA_TO_POLARS = {\n    \"text\": pl.Utf8,\n    \"number\": pl.Float64, \n    \"calendar_date\": pl.Datetime(\"us\"),\n    \"floating_timestamp\": pl.Datetime(\"us\"),  \n}\n\n\n# Loads the schema (i.e., types) of our data set\n6@data_loader\ndef load_data_schema_from_api(*args, **kwargs):\n\n7    url = 'https://{DOMAIN}/api/views/{DATASET_ID}'.format(**kwargs)\n    response = requests.get(url)\n\n    metadata = response.json()\n    columns = metadata.get(\"columns\", [])\n    schema = {\n8        col[\"fieldName\"]: SODA_TO_POLARS.get(col[\"dataTypeName\"], pl.Utf8)\n        for col in columns\n9        if not col[\"fieldName\"].startswith(\":@computed_\")\n        }\n    \n    return schema\n\n10@test\ndef test_output(dictionary, *args) -&gt; None:\n    assert dictionary is not None, \"The output is undefined\"\n    assert isinstance(dictionary, dict), \"The output is not a dictionary\"\n    assert len(dictionary) &gt; 0, \"The dictionary is empty\"\n\n\n1\n\nPolars: used for data manipulation and type mapping throughout the project.\n\n2\n\nHTTP library: requests is used to send HTTP requests to the API.\n\n3\n\nLoader decorator: the @data_loader decorator marks the function as a Mage data loader block.\n\n4\n\nTest decorator: the @test decorator defines a test for validating the function‚Äôs output.\n\n5\n\nSocrata to Polars type mapping: the SODA_TO_POLARS dictionary maps Socrata data types to corresponding Polars types, ensuring compatibility.\n\n6\n\nFunction decoration: the loader function load_data_schema_from_api is decorated with @data_loader to integrate it into the Mage pipeline.\n\n7\n\nEndpoint definition: The endpoint URL is dynamically generated using the global variables DOMAIN and DATASET_ID. This is where the schema metadata is fetched.\n\n8\n\nDictionary comprehension: the schema dictionary maps column names to their corresponding Polars types, based on the SODA_TO_POLARS dictionary. Unrecognized types default to Utf8.\n\n9\n\nExclusion columns: Columns with names starting with :@computed_ are excluded from the schema.\n\n10\n\nTest function: the @test decorator validates the loader‚Äôs output, ensuring it is a non-empty dictionary of the correct type.\n\n\n\n\nThe primary goal of the get_schema_from_metadata block is to retrieve data type information to standardize batch ingestion. While Polars can infer data types based on the content they read, each batch may differ in structure. This inconsistency can lead to errors and prevent us from merging batches later. We can resolve this issue by specifying data types and ensuring a consistent schema across all batches.\nPlease notice that we use Polars for data transformation and manipulation. Compared with Pandas, Polars offers better performance and scalability, making it a more cost-effective choice for production systems. This case study demonstrates that Polars can reduce computational costs, making it a good choice for projects that may eventually transition to production. Polars is a mature and robust library with extensive capabilities, making it an excellent choice for this project.\nWith our first block created, we can explain how the Mage flow works. First‚Ä¶ to finally pass the resulting data to the block downstream\n[IMAGE]\n\nimport os\nimport duckdb\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndb_path = 'data/iowa_liquor.duckdb'\n\n@custom\ndef check_last_year(*args, **kwargs):\n    \"\"\"\n    Rethrieves the latest year in our local DuckDB database. If the table does not exist, it returns 0.\n\n    Returns:\n        last_year (int): year of the latest record in our local database.\n    \"\"\"\n    conn = duckdb.connect(\"data/iowa_liquor.duckdb\")\n    \n    try:\n        result = conn.execute(\"\"\"\n        SELECT EXTRACT(YEAR FROM MAX(date)) FROM iowa_liquor_sales\n        \"\"\").fetchall()\n        last_year = result[0][0]\n\n    except duckdb.CatalogException as e:\n        print(\"The table doesn't exist; assigning rows_in_db=0\")\n        last_year = 0\n\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        last_year = 0\n\n    # Close DB connections\n    conn.close()\n\n    return last_year\n\n\n \n@test\ndef test_output(output, *args) -&gt; None:\n    \"\"\"\n    Test if there is an output and verifies its type.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert isinstance(output, int), 'The output is not int'\n\nget_schema_from_metadata\nsoda_records_per_year\ncheck_local_db\nBash execution\n\nuser@user-pc:~/mage_duckdb_pipeline$ mage run . socrata_iowa_liquor_pipeline\nFetching the records-per-year metadata. This might take a couple minutes..\nDone! We have our year record metadata.\n\nSODA data pull started.\nYears to be fetched: 2017, 2018, 2019, 2020, 2021.\nFetching data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [10:08&lt;00:00, 121.69s/it]\nProduct-related new variables, generated.\nSales and price related metrics, computed.\nVolume-based features, computed.\nTime-based features, computed.\nData loaded to your DuckDB database!\nPipeline run completed.\n\n\nconn2 &lt;- DBI::dbConnect(\n  drv = duckdb::duckdb(),\n  dbdir = \"../data/iowa_liquor.duckdb\",\n  read_only = TRUE\n)\n\n\nSELECT *\nFROM iowa_liquor_sales\nLIMIT 10\n\n\ntibble::tibble(query2)\n\n# A tibble: 10 √ó 36\n   invoice_line_no date                store name          address city  zipcode\n   &lt;chr&gt;           &lt;dttm&gt;              &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  \n 1 S10440300006    2013-02-05 00:00:00 2655  HY-VEE FOOD ‚Ä¶ 1200 S‚Ä¶ CLAR‚Ä¶ 51632  \n 2 S16464700176    2013-12-23 00:00:00 2638  HY-VEE WINE ‚Ä¶ 5925 U‚Ä¶ CEDA‚Ä¶ 50613  \n 3 S12001000067    2013-05-02 00:00:00 2561  HY-VEE FOOD ‚Ä¶ 4605 F‚Ä¶ DES ‚Ä¶ 50321  \n 4 S12643000007    2013-06-06 00:00:00 3973  MMDG SPIRITS‚Ä¶ 126A W‚Ä¶ AMES  50014  \n 5 S14029600009    2013-08-21 00:00:00 3776  WAL-MART 511‚Ä¶ 3101 W‚Ä¶ DAVE‚Ä¶ 52806  \n 6 S15244000066    2013-10-21 00:00:00 2190  CENTRAL CITY‚Ä¶ 1460 2‚Ä¶ DES ‚Ä¶ 50314  \n 7 S10238500019    2013-01-24 00:00:00 4023  WAL-MART 138‚Ä¶ 1515 S‚Ä¶ BOONE 50036  \n 8 S13319200017    2013-07-11 00:00:00 4426  LIQUOR AND G‚Ä¶ 114 CE‚Ä¶ MARS‚Ä¶ 50158  \n 9 S14901500074    2013-10-02 00:00:00 4129  CYCLONE LIQU‚Ä¶ 626 LI‚Ä¶ AMES  50010  \n10 S15397800035    2013-10-29 00:00:00 2565  HY-VEE FOOD ‚Ä¶ 819 N ‚Ä¶ SPEN‚Ä¶ 51301  \n# ‚Ñπ 29 more variables: store_location &lt;chr&gt;, county_number &lt;chr&gt;, county &lt;chr&gt;,\n#   category &lt;chr&gt;, category_name &lt;chr&gt;, vendor_no &lt;chr&gt;, vendor_name &lt;chr&gt;,\n#   itemno &lt;chr&gt;, im_desc &lt;chr&gt;, pack &lt;dbl&gt;, bottle_volume_ml &lt;dbl&gt;,\n#   state_bottle_cost &lt;dbl&gt;, state_bottle_retail &lt;dbl&gt;, sale_bottles &lt;dbl&gt;,\n#   sale_dollars &lt;dbl&gt;, sale_liters &lt;dbl&gt;, sale_gallons &lt;dbl&gt;,\n#   liquor_type &lt;chr&gt;, is_premium &lt;lgl&gt;, bottle_size &lt;chr&gt;,\n#   gov_profit_margin &lt;dbl&gt;, gov_retail_markup_percentage &lt;dbl&gt;, ‚Ä¶\n\n\n\nimport duckdb\n\ncon_py = duckdb.connect(\"../data/iowa_liquor.duckdb\", read_only=True)\n\npolars_df = con_py.sql(\"SELECT liquor_type, bottle_size, price_per_liter FROM iowa_liquor_sales LIMIT 10\").pl()\n\npolars_df\n\n\nshape: (10, 3)\n\n\n\nliquor_type\nbottle_size\nprice_per_liter\n\n\nstr\nstr\nf32\n\n\n\n\n\"Whisky\"\n\"large\"\n16.651428\n\n\n\"Whisky\"\n\"medium\"\n29.92\n\n\n\"Cream\"\n\"medium\"\n25.0\n\n\n\"Rum\"\n\"large\"\n10.91\n\n\n\"Whisky\"\n\"medium\"\n42.0\n\n\n\"Whisky\"\n\"large\"\n11.994286\n\n\n\"Rum\"\n\"large\"\n6.497143\n\n\n\"Whisky\"\n\"medium\"\n25.879999\n\n\n\"Gin\"\n\"medium\"\n10.013333\n\n\n\"Vodka\"\n\"medium\"\n26.24\n\n\n\n\n\n\n\npolars_df.height\n\n10"
  }
]